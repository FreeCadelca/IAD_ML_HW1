{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffq6A2-ifzAA"
   },
   "source": [
    "# Интеллектуальный анализ данных – весна 2025\n",
    "# Домашнее задание 6: классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Правила:\n",
    "\n",
    "\n",
    "\n",
    "*   Домашнее задание оценивается в 10 баллов.\n",
    "*   Можно использовать без доказательства любые результаты, встречавшиеся на лекциях или семинарах по курсу, если получение этих результатов не является вопросом задания.\n",
    "*  Можно использовать любые свободные источники с *обязательным* указанием ссылки на них.\n",
    "*  Плагиат не допускается. При обнаружении случаев списывания, 0 за работу выставляется всем участникам нарушения, даже если можно установить, кто у кого списал.\n",
    "*  Старайтесь сделать код как можно более оптимальным. В частности, будет штрафоваться использование циклов в тех случаях, когда операцию можно совершить при помощи инструментов библиотек, о которых рассказывалось в курсе.\n",
    "* Если в задании есть вопрос на рассуждение, то за отсутствие ответа на него балл за задание будет снижен вполовину."
   ],
   "metadata": {
    "id": "EPcxtekTA1Sm"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itRtFtrOf0_b"
   },
   "source": [
    "В этом домашнем задании вам предстоит построить классификатор текстов.\n",
    "\n",
    "Будем предсказывать эмоциональную окраску твиттов о коронавирусе.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tNGRVO7_g9mz",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:12.061048900Z",
     "start_time": "2025-05-11T18:58:12.025692800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zOy8iHJQg_Ss",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "outputId": "6a32c325-1b9a-4895-ab22-5e985016da91",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:12.135052Z",
     "start_time": "2025-05-11T18:58:12.038411200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       UserName  ScreenName           Location     TweetAt  \\\n9251      15024       59976  San Francisco, CA  20-03-2020   \n18621     26469       71421                NaN  25-03-2020   \n6638      11865       56817                NaN  19-03-2020   \n21051     29472       74424   Toronto, Ontario  01-04-2020   \n\n                                           OriginalTweet           Sentiment  \n9251   A huge shout out to all of the farmers, grocer...  Extremely Positive  \n18621  https://t.co/ndpWYJ20Iw CONSUMER ALERT: New CO...            Negative  \n6638   ItÃÂs ÃÂunBritishÃÂ to apply restriction...            Positive  \n21051  @CNN oil prices..... 800+ dead today ....... d...            Negative  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9251</th>\n      <td>15024</td>\n      <td>59976</td>\n      <td>San Francisco, CA</td>\n      <td>20-03-2020</td>\n      <td>A huge shout out to all of the farmers, grocer...</td>\n      <td>Extremely Positive</td>\n    </tr>\n    <tr>\n      <th>18621</th>\n      <td>26469</td>\n      <td>71421</td>\n      <td>NaN</td>\n      <td>25-03-2020</td>\n      <td>https://t.co/ndpWYJ20Iw CONSUMER ALERT: New CO...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>6638</th>\n      <td>11865</td>\n      <td>56817</td>\n      <td>NaN</td>\n      <td>19-03-2020</td>\n      <td>ItÃÂs ÃÂunBritishÃÂ to apply restriction...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>21051</th>\n      <td>29472</td>\n      <td>74424</td>\n      <td>Toronto, Ontario</td>\n      <td>01-04-2020</td>\n      <td>@CNN oil prices..... 800+ dead today ....... d...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_coronavirus.csv', encoding='latin-1')\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2OiDog9ZBlS"
   },
   "source": [
    "Для каждого твитта указано:\n",
    "\n",
    "\n",
    "*   UserName - имя пользователя, заменено на целое число для анонимности\n",
    "*   ScreenName - отображающееся имя пользователя, заменено на целое число для анонимности\n",
    "*   Location - местоположение\n",
    "*   TweetAt - дата создания твитта\n",
    "*   OriginalTweet - текст твитта\n",
    "*   Sentiment - эмоциональная окраска твитта (целевая переменная)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZTMseDkhTC7"
   },
   "source": [
    "## Задание 1 Подготовка (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx2-odn9hdAW"
   },
   "source": [
    "Целевая переменная находится в колонке `Sentiment`.  Преобразуйте ее таким образом, чтобы она стала бинарной: 1 - если у твитта положительная или очень положительная эмоциональная окраска и 0 - если отрицательная или очень отрицательная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZaQKQ1zEjP15",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:12.159493700Z",
     "start_time": "2025-05-11T18:58:12.134051500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       UserName  ScreenName        Location     TweetAt  \\\n15816     23028       67980   Canada & USA   24-03-2020   \n13317     19963       64915             NaN  22-03-2020   \n23861     32960       77912  United Kingdom  05-04-2020   \n31700     42796       87748             NaN  12-04-2020   \n\n                                           OriginalTweet  Sentiment  \n15816  So proud of this Blog - I think the best we ha...          1  \n13317  Coles Woolies Farmers exploit nation $9 mini C...          0  \n23861  @Ronnie2K With COVID-19, every business is dox...          1  \n31700  The measures were in response to COVID-19 pand...          0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15816</th>\n      <td>23028</td>\n      <td>67980</td>\n      <td>Canada &amp; USA</td>\n      <td>24-03-2020</td>\n      <td>So proud of this Blog - I think the best we ha...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13317</th>\n      <td>19963</td>\n      <td>64915</td>\n      <td>NaN</td>\n      <td>22-03-2020</td>\n      <td>Coles Woolies Farmers exploit nation $9 mini C...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23861</th>\n      <td>32960</td>\n      <td>77912</td>\n      <td>United Kingdom</td>\n      <td>05-04-2020</td>\n      <td>@Ronnie2K With COVID-19, every business is dox...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>31700</th>\n      <td>42796</td>\n      <td>87748</td>\n      <td>NaN</td>\n      <td>12-04-2020</td>\n      <td>The measures were in response to COVID-19 pand...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'] = df['Sentiment'].apply(lambda x: 1 if x in ('Positive', 'Extremely Positive') else 0)\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGq1FxJ-kBo5"
   },
   "source": [
    "Сбалансированы ли классы?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a7gdNtxckK5V",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:12.162036Z",
     "start_time": "2025-05-11T18:58:12.146863900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "1    18046\n",
      "0    15398\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng8BCelMkWb0"
   },
   "source": [
    "**Ответ:** Классы сбалансированы достаточно, но не идеально"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmSIBSsLk5Zz"
   },
   "source": [
    "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их строкой 'Unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UhUVRkR5kxa7",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:12.246773400Z",
     "start_time": "2025-05-11T18:58:12.152383900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data in columns before fill:\n",
      "UserName 0\n",
      "ScreenName 0\n",
      "Location 7049\n",
      "TweetAt 0\n",
      "OriginalTweet 0\n",
      "Sentiment 0\n",
      "Missing data in columns after fill:\n",
      "UserName 0\n",
      "ScreenName 0\n",
      "Location 0\n",
      "TweetAt 0\n",
      "OriginalTweet 0\n",
      "Sentiment 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing data in columns before fill:\")\n",
    "for i in df.head():\n",
    "    print(i, df[i].isna().sum())\n",
    "for i in df.head():\n",
    "    df[i].fillna(\"Unknown\", inplace=True)\n",
    "print(\"Missing data in columns after fill:\")\n",
    "for i in df.head():\n",
    "    print(i, df[i].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tzt27tfjUpq"
   },
   "source": [
    "Разделите данные на обучающие и тестовые в соотношении 7 : 3 и укажите `random_state=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xSLOA9tIj9Z6",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:13.030234900Z",
     "start_time": "2025-05-11T18:58:12.167841600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.3, stratify=df['Sentiment'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9RrPUsJlL60"
   },
   "source": [
    "## Задание 2 Токенизация (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Dz_b7Xopc_R"
   },
   "source": [
    "Постройте словарь на основе обучающей выборки и посчитайте количество встреч каждого токена с использованием самой простой токенизации - деления текстов по пробельным символам и приведения токенов в нижний регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SFr67WOJphny",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:13.317464100Z",
     "start_time": "2025-05-11T18:58:13.095355400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    advice Talk to your neighbours family to excha...\n",
      "1    Coronavirus Australia: Woolworths to give elde...\n",
      "2    My food stock is not the only one which is emp...\n",
      "3    Me, ready to go at supermarket during the #COV...\n",
      "4    As news of the regionÃÂs first confirmed COV...\n",
      "Name: OriginalTweet, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": "              Word  Count\n0              the  38250\n1               to  33447\n2              and  20935\n3               of  18578\n4                a  16667\n...            ...    ...\n103195  @tartiicat      1\n103196    new/used      1\n103197        rift      1\n103198     $700.00      1\n103199      whethe      1\n\n[103200 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the</td>\n      <td>38250</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>to</td>\n      <td>33447</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>and</td>\n      <td>20935</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>of</td>\n      <td>18578</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a</td>\n      <td>16667</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>103195</th>\n      <td>@tartiicat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103196</th>\n      <td>new/used</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103197</th>\n      <td>rift</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103198</th>\n      <td>$700.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103199</th>\n      <td>whethe</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>103200 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "print(df[\"OriginalTweet\"].head(5))\n",
    "\n",
    "for i in df[\"OriginalTweet\"]:\n",
    "    for word in i.split():\n",
    "        words.append(word.lower())\n",
    "\n",
    "words = Counter(words)\n",
    "words_count = words.most_common()\n",
    "words_count_df = pd.DataFrame(words_count, columns=['Word', 'Count'])\n",
    "\n",
    "words_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe0h2Jqkpnao"
   },
   "source": [
    "Какой размер словаря получился?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:** Как мы видим, получилось 103200 строк, значит столько же различных токенов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d2G1Z-Qpqkd"
   },
   "source": [
    "Выведите 10 самых популярных токенов с количеством встреч каждого из них. Объясните, почему именно эти токены в топе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Impi32a_pssg",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:13.322843200Z",
     "start_time": "2025-05-11T18:58:13.313476700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           Word  Count\n0           the  38250\n1            to  33447\n2           and  20935\n3            of  18578\n4             a  16667\n5            in  16024\n6           for  12193\n7  #coronavirus  11759\n8            is  10596\n9           are   9958",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the</td>\n      <td>38250</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>to</td>\n      <td>33447</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>and</td>\n      <td>20935</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>of</td>\n      <td>18578</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a</td>\n      <td>16667</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>in</td>\n      <td>16024</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>for</td>\n      <td>12193</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#coronavirus</td>\n      <td>11759</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>is</td>\n      <td>10596</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>are</td>\n      <td>9958</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtuJCD0ApuFd"
   },
   "source": [
    "**Ответ:** Логично, что артикли, предлоги и ```#coronavirus``` оказались в топе, потому что артикли и предлоги - сами по себе частые связки в тексте, а хештег - просто тематика всех текстов, логично, что такой хештег будет "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7DTQDkWsVYp"
   },
   "source": [
    "Удалите стоп-слова из словаря и выведите новый топ-10 токенов (и количество встреч) по популярности.  Что можно сказать  о нем?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8csSAdgTsnFx",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:14.363933500Z",
     "start_time": "2025-05-11T18:58:13.318464100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            Word  Count\n7   #coronavirus  11759\n16        prices   5625\n18          food   5409\n23       grocery   4882\n24   supermarket   4662\n25      covid-19   4504\n26        people   4488\n27         store   4486\n35      #covid19   3561\n39      consumer   3233",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>#coronavirus</td>\n      <td>11759</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>prices</td>\n      <td>5625</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>food</td>\n      <td>5409</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>grocery</td>\n      <td>4882</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>supermarket</td>\n      <td>4662</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>covid-19</td>\n      <td>4504</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>people</td>\n      <td>4488</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>store</td>\n      <td>4486</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>#covid19</td>\n      <td>3561</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>consumer</td>\n      <td>3233</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stops = set(stopwords.words('english'))\n",
    "words_count_df_copy = words_count_df[~words_count_df[\"Word\"].isin(stops)]\n",
    "words_count_df_copy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZH0x2Lzs-Dh"
   },
   "source": [
    "**Ответ:** Видим, что теперь токены очистились от мусора - артиклей, предлогов и местоимений, которые, логично, есть в каждом тексте, и токены отражают обычные хештеги или самые частые слова. Я вижу, что многие из них связаны с едой или пищевой промышленностью. Возможно, это произошло, потому что еда - главное в выживании во времена коронавируса, а также супермаркеты - неизбежное скопление людей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKSGRyI-uor0"
   },
   "source": [
    "Также выведите 20 самых непопулярных слов (если самых непопулярных слов больше, выведите любые 20 из них) Почему эти токены непопулярны, требуется ли как-то дополнительно работать с ними?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "moArbwfvun9t",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:14.368445800Z",
     "start_time": "2025-05-11T18:58:14.362934200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                           Word  Count\n103180                bullion's      1\n103181              #safe-haven      1\n103182          $1,715.25/ounce      1\n103183                     dec.      1\n103184                $1,722.20      1\n103185                rs.46,215      1\n103186                     gms.      1\n103187  https://t.co/s8coy5vvgn      1\n103188                   home??      1\n103189  https://t.co/v8xdxhqeyn      1\n103190           @mrsilverscott      1\n103191                  delays.      1\n103192                rejecting      1\n103193            @kameronwilds      1\n103194            martinsville,      1\n103195               @tartiicat      1\n103196                 new/used      1\n103197                     rift      1\n103198                  $700.00      1\n103199                   whethe      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>103180</th>\n      <td>bullion's</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103181</th>\n      <td>#safe-haven</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103182</th>\n      <td>$1,715.25/ounce</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103183</th>\n      <td>dec.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103184</th>\n      <td>$1,722.20</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103185</th>\n      <td>rs.46,215</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103186</th>\n      <td>gms.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103187</th>\n      <td>https://t.co/s8coy5vvgn</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103188</th>\n      <td>home??</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103189</th>\n      <td>https://t.co/v8xdxhqeyn</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103190</th>\n      <td>@mrsilverscott</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103191</th>\n      <td>delays.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103192</th>\n      <td>rejecting</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103193</th>\n      <td>@kameronwilds</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103194</th>\n      <td>martinsville,</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103195</th>\n      <td>@tartiicat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103196</th>\n      <td>new/used</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103197</th>\n      <td>rift</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103198</th>\n      <td>$700.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103199</th>\n      <td>whethe</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_df_copy.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRp3J1gQunlR"
   },
   "source": [
    "**Ответ:** Эти токены непопулярны, потому что содержат пунктуацию, ссылки, конкретные числа/цифры или непопулярные хештеги. Я считаю, что их нужно обработать, удалив пунктуацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wx9LQOSPzvjV"
   },
   "source": [
    "Теперь воспользуемся токенайзером получше - TweetTokenizer из библиотеки nltk. Примените его и посмотрите на топ-10 популярных слов. Чем он отличается от топа, который получался раньше? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2G1UkyVxzvFY",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:17.730836800Z",
     "start_time": "2025-05-11T18:58:14.368445800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               Word  Count\n0               the  34781\n1                 .  34284\n2                to  32812\n3                 ,  25142\n4               and  20439\n...             ...    ...\n87328  martinsville      1\n87329    @TartiiCat      1\n87330          Rift      1\n87331        700.00      1\n87332        whethe      1\n\n[87333 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the</td>\n      <td>34781</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>.</td>\n      <td>34284</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>to</td>\n      <td>32812</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>,</td>\n      <td>25142</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>and</td>\n      <td>20439</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>87328</th>\n      <td>martinsville</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87329</th>\n      <td>@TartiiCat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87330</th>\n      <td>Rift</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87331</th>\n      <td>700.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87332</th>\n      <td>whethe</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>87333 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "words = []\n",
    "\n",
    "for i in df[\"OriginalTweet\"]:\n",
    "    words += tokenizer.tokenize(i)\n",
    "\n",
    "words = Counter(words)\n",
    "words_count = words.most_common()\n",
    "words_count_df = pd.DataFrame(words_count, columns=['Word', 'Count'])\n",
    "\n",
    "words_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50eVUnJN1Zxl"
   },
   "source": [
    "**Ответ:** На удивление, сейчас токенайзер выдал даже более худшие результаты, чем \"рабоче-крестьянский\" метод с split, потому что наш новый токенайзер отделяет знаки пунктуации от основного текста, и, естественно, знаков пунктуации в топе много."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gqQgiMs11bs"
   },
   "source": [
    "Удалите из словаря стоп-слова и пунктуацию, посмотрите на новый топ-10 слов с количеством встреч, есть ли теперь в нем что-то не похожее на слова?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0yHWdFrp0Mup",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:17.749226700Z",
     "start_time": "2025-05-11T18:58:17.729291600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               Word  Count\n10                Â  10498\n11                  10361\n13     #coronavirus  10211\n14               19  10142\n18                I   7484\n...             ...    ...\n87328  martinsville      1\n87329    @TartiiCat      1\n87330          Rift      1\n87331        700.00      1\n87332        whethe      1\n\n[87110 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>Â</td>\n      <td>10498</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td></td>\n      <td>10361</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>#coronavirus</td>\n      <td>10211</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>19</td>\n      <td>10142</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>I</td>\n      <td>7484</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>87328</th>\n      <td>martinsville</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87329</th>\n      <td>@TartiiCat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87330</th>\n      <td>Rift</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87331</th>\n      <td>700.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87332</th>\n      <td>whethe</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>87110 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "punctuation_list = list(punctuation)\n",
    "words_count_df_copy = words_count_df[\n",
    "    ~words_count_df[\"Word\"].isin(punctuation_list) & ~words_count_df[\"Word\"].isin(stops)]\n",
    "words_count_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZJqXELP_Yxy"
   },
   "source": [
    "**Ответ:** Да, после очистки от мусора можно видеть а-ля адекватный список топа слов. Однако все еще присутствует мусор - нечитаемые символы, возможно, служебные, возможно, смайлики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzXjMsSB_kXB"
   },
   "source": [
    "Скорее всего в некоторых топах были неотображаемые символы или отдельные буквы не латинского алфавита. Уберем их: удалите из словаря токены из одного символа, позиция которого в таблице Unicode 128 и более (`ord(x) >= 128`)\n",
    "\n",
    "Выведите топ-10 самых популярных и топ-20 непопулярных слов. Чем полученные топы отличаются от итоговых топов, полученных при использовании токенизации по пробелам? Что теперь лучше, а что хуже?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1695hlkS_1-J",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:17.866390700Z",
     "start_time": "2025-05-11T18:58:17.748207900Z"
    }
   },
   "outputs": [],
   "source": [
    "words_count_df_copy = words_count_df_copy[\n",
    "    (words_count_df_copy[\"Word\"].str.len() > 1) |\n",
    "    (words_count_df_copy[\"Word\"].apply(\n",
    "        lambda x: ord(x) if len(x) == 1 else 0\n",
    "    ) <= 127)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "            Word  Count\n13  #coronavirus  10211\n14            19  10142\n18             I   7484\n23        prices   6166\n24         COVID   5945\n26          food   5423\n29         store   5234\n33   supermarket   4803\n35       grocery   4350\n36        people   4300",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td>#coronavirus</td>\n      <td>10211</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>19</td>\n      <td>10142</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>I</td>\n      <td>7484</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>prices</td>\n      <td>6166</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>COVID</td>\n      <td>5945</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>food</td>\n      <td>5423</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>store</td>\n      <td>5234</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>supermarket</td>\n      <td>4803</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>grocery</td>\n      <td>4350</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>people</td>\n      <td>4300</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_df_copy.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:17.904442900Z",
     "start_time": "2025-05-11T18:58:17.796828200Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                          Word  Count\n87313  https://t.co/qUQ8Y0uM6n      1\n87314             @MajangChien      1\n87315                bullion's      1\n87316              #safe-haven      1\n87317                    1,715      1\n87318                    1,722      1\n87319                   46,215      1\n87320                      gms      1\n87321  https://t.co/S8coY5VVgN      1\n87322  https://t.co/v8XDXhqeYN      1\n87323                 roulette      1\n87324           @MrSilverScott      1\n87325                rejecting      1\n87326                      TAT      1\n87327            @KameronWilds      1\n87328             martinsville      1\n87329               @TartiiCat      1\n87330                     Rift      1\n87331                   700.00      1\n87332                   whethe      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>87313</th>\n      <td>https://t.co/qUQ8Y0uM6n</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87314</th>\n      <td>@MajangChien</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87315</th>\n      <td>bullion's</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87316</th>\n      <td>#safe-haven</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87317</th>\n      <td>1,715</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87318</th>\n      <td>1,722</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87319</th>\n      <td>46,215</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87320</th>\n      <td>gms</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87321</th>\n      <td>https://t.co/S8coY5VVgN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87322</th>\n      <td>https://t.co/v8XDXhqeYN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87323</th>\n      <td>roulette</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87324</th>\n      <td>@MrSilverScott</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87325</th>\n      <td>rejecting</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87326</th>\n      <td>TAT</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87327</th>\n      <td>@KameronWilds</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87328</th>\n      <td>martinsville</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87329</th>\n      <td>@TartiiCat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87330</th>\n      <td>Rift</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87331</th>\n      <td>700.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87332</th>\n      <td>whethe</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_df_copy.tail(20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:17.932029Z",
     "start_time": "2025-05-11T18:58:17.799317600Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzjHAKIlDvc6"
   },
   "source": [
    "**Ответ:** Теперь топ похож на реальный, немного отличается от нашего прошлого способа (в том числе, потому что мы не приводили слова к нижнему регистру). Из плюсов: разделения стали лучше, потому что не учитывается пунктуация и маленькие различия между словами, из минусов: появились местоимения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcDf9_6HB2zm"
   },
   "source": [
    "Выведите топ-10 популярных хештегов (токены, первые символы которых - #) с количеством встреч. Что можно сказать о них?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zk4fygCUBw3l",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:18.091490900Z",
     "start_time": "2025-05-11T18:58:17.805230200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Temp\\ipykernel_2972\\533891431.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  words_count_df_copy[words_count_df[\"Word\"].str[0] == '#'].head(10)\n"
     ]
    },
    {
     "data": {
      "text/plain": "              Word  Count\n13    #coronavirus  10211\n60        #COVID19   2621\n75       #Covid_19   2126\n90    #Coronavirus   1806\n116     #COVID2019   1341\n159   #toiletpaper    944\n175       #covid19    829\n186         #COVID    775\n246  #CoronaCrisis    599\n284   #CoronaVirus    525",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td>#coronavirus</td>\n      <td>10211</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>#COVID19</td>\n      <td>2621</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>#Covid_19</td>\n      <td>2126</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>#Coronavirus</td>\n      <td>1806</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>#COVID2019</td>\n      <td>1341</td>\n    </tr>\n    <tr>\n      <th>159</th>\n      <td>#toiletpaper</td>\n      <td>944</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>#covid19</td>\n      <td>829</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>#COVID</td>\n      <td>775</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>#CoronaCrisis</td>\n      <td>599</td>\n    </tr>\n    <tr>\n      <th>284</th>\n      <td>#CoronaVirus</td>\n      <td>525</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_df_copy[words_count_df[\"Word\"].str[0] == '#'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6NeNWBkDxM7"
   },
   "source": [
    "**Ответ:** О хештегах можно сказать следующее: все хештеги, очевидно, упоминают эпидемию. Один забавный хештег смог затесаться в этот топ - #toiletpaper, все мы знаем, почему :))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLYBg7caD5GA"
   },
   "source": [
    "То же самое проделайте для ссылок на сайт https://t.co Сравнима ли популярность ссылок с популярностью хештегов? Будет ли информация о ссылке на конкретную страницу полезна?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MXbm1oeaCK9S",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:18.094409500Z",
     "start_time": "2025-05-11T18:58:17.835485900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Word  Count\n",
      "11014  https://t.co/oXA7SWtoNd      6\n",
      "12562  https://t.co/G63RP042HO      5\n",
      "13125  https://t.co/R7sAGojsjg      4\n",
      "13354  https://t.co/WrLHYzIzAA      4\n",
      "13709  https://t.co/ymsEmlVTTd      4\n",
      "13993  https://t.co/3kFUIOJXEp      4\n",
      "14604  https://t.co/OI39zSAnQ8      4\n",
      "14916  https://t.co/6yVYKIAb2c      4\n",
      "14917  https://t.co/xPcm2Xkj4O      4\n",
      "14951  https://t.co/gu6B4XpqP4      4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Temp\\ipykernel_2972\\2305966469.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  print(words_count_df_copy[words_count_df[\"Word\"].str.startswith('https://t.co')].head(10))\n"
     ]
    }
   ],
   "source": [
    "print(words_count_df_copy[words_count_df[\"Word\"].str.startswith('https://t.co')].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at6lRYZ8A07N"
   },
   "source": [
    "**Ответ:** Так как топ весьма маленький (мало повторений) и по ссылкам не всегда понятна окраска сообщения, в котором была вставлена ссылка, то информация, которую она несёт, в том виде, в котором мы видим сейчас, невелика."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOGdUU1kBU1D"
   },
   "source": [
    "Используем опыт предыдущих экспериментов и напишем собственный токенайзер, улучшив TweetTokenizer. Функция tokenize должна:\n",
    "\n",
    "\n",
    "\n",
    "*   Привести текст в нижний регистр\n",
    "*   Применить TweetTokenizer для  выделения токенов\n",
    "*   Удалить стоп-слова, пунктуацию, токены из одного символа с позицией в таблице Unicode 128 и более,  ссылки на t.co\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ctEsB6xkFrrK",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:18.095495Z",
     "start_time": "2025-05-11T18:58:17.858874700Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    words_df = pd.DataFrame(words, columns=['Word'])\n",
    "    words_df = words_df[~words_df[\"Word\"].isin(punctuation_list) & ~words_df[\"Word\"].isin(stops)]\n",
    "\n",
    "    words_df = words_df[\n",
    "        (words_df[\"Word\"].str.len() > 1) |\n",
    "        (words_df[\"Word\"].apply(\n",
    "            lambda x: ord(x) if len(x) == 1 else 0\n",
    "        ) <= 127)\n",
    "        ]\n",
    "    \n",
    "    words_df = words_df.loc[~words_df[\"Word\"].str.startswith('https://t.co')]\n",
    "    \n",
    "    return words_df['Word'].array.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwbgtYkJGYym",
    "outputId": "5808765b-3448-45e6-ccc1-7cd65f6371ef",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:18.116877200Z",
     "start_time": "2025-05-11T18:58:17.860388700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['sample', 'text', '@sample_text', '#sampletext']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer('This is sample text!!!! @Sample_text I, \\x92\\x92 https://t.co/sample  #sampletext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wURVABmXHk97"
   },
   "source": [
    "## Задание 3 Векторизация текстов (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H44iXkoHIQfN"
   },
   "source": [
    "Обучите CountVectorizer с использованием custom_tokenizer в качестве токенайзера. Как размер полученного словаря соотносится с размером изначального словаря из начала задания 2?"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = df[\"OriginalTweet\"].array.tolist()\n",
    "# print(corpus[:10])\n",
    "cv = CountVectorizer(tokenizer=custom_tokenizer) \n",
    "cv.fit(corpus)\n",
    "\n",
    "print(len(cv.vocabulary_))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHn_limQl3BI",
    "outputId": "8e9c1826-319f-4376-f06e-c30c2eb82648",
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:48.559333100Z",
     "start_time": "2025-05-11T18:58:17.866390700Z"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56523\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsfmaSGoItUm"
   },
   "source": [
    "**Ответ:** Было ~87K, стало 56K, значит мы отсеяли больше мусора, чем могли, значит сделали хорошую работу. Однако в авторские числа я так и не смог попасть. Возможно, из-за обновления списка стоп-слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm6UHNmqKZT0"
   },
   "source": [
    "Посмотрим на какой-нибудь конкретный твитт:"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       UserName                                      OriginalTweet  Sentiment\n18930     26841  ÃÂand Martin upset some of his workers by te...          0\n30267     40983  Spot on Consumer Consumers also face liquidity...          0\n18931     26842  ÃÂWe can all now see that jobs that are neve...          0\n5399      10358  @3DAdept \\r\\r\\nLow-cost open source ventilator...          0\n33024     44417  French consumersÃÂ begin to anticipate reces...          0\n15029     22072  From musicians left without a source of income...          0\n11656     17935  No panic buying in our local @Leclerc supermar...          0\n634        4549  @EBBerger (What shocked me in this thread is t...          0\n24637     33945  L'ÃÂ©pidÃÂ©mie de #coronavirus va accÃÂ©lÃ...          0\n7942      13451  #coronavirus\\r\\r\\nTearful nurse urges the publ...          0\n8834      14533  #Coronavirus fatalities are rising in #Europe,...          0\n27970     38097  I'll be posting regular updates on @OECD's pol...          0\n24262     33467  The recent oil prices crash summed up in 1 twe...          0\n3885       8514  $MAJOR #Major Cineplex Group PCL Major Cineple...          0\n11543     17795  Our local french supermarket Fully stocked no ...          0\n15184     22283  Covid-19: Blackstone buys logistics assets as ...          0\n15120     22190  $AF #Air France-KLM ... ODDO : COVID-19: our t...          0\n1775       5961  $PDX #Paradox Interactive AB Paradox Interacti...          0\n523        4428  Covid19 panic. Pascal Montagne for @37degres a...          0\n1911       6127  #Coronavirus Fears May Drive U.S. #eCommerce S...          0\n31838     42965  Chocolatiers slash prices as shoppers in #coro...          0\n5815      10864  5 The world Don t panic and store food Idlib P...          0\n27728     37786  Your patient with hypertension asks you for a ...          0\n23228     32182  One example of the complete IRRATIONALITY of t...          0\n28405     38640  Motilal Oswal - MOSL: Morning India (8/April/2...          0\n26013     35666  To counterbalance the loss of revenue with the...          0\n32199     43405  ÃÂ«ÃÂ Industrial real-estate operators expec...          0\n966        4964  @DrAdrianHeald Did my weekly shop in local Fre...          0\n12145     18527  SPOTTED Angela Merkel shopping in local superm...          0\n31411     42446  The shift in demand has been a problem for fis...          0\n27933     38046  Every department store in America has tried so...          0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18930</th>\n      <td>26841</td>\n      <td>ÃÂand Martin upset some of his workers by te...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30267</th>\n      <td>40983</td>\n      <td>Spot on Consumer Consumers also face liquidity...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18931</th>\n      <td>26842</td>\n      <td>ÃÂWe can all now see that jobs that are neve...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5399</th>\n      <td>10358</td>\n      <td>@3DAdept \\r\\r\\nLow-cost open source ventilator...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33024</th>\n      <td>44417</td>\n      <td>French consumersÃÂ begin to anticipate reces...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15029</th>\n      <td>22072</td>\n      <td>From musicians left without a source of income...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11656</th>\n      <td>17935</td>\n      <td>No panic buying in our local @Leclerc supermar...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>634</th>\n      <td>4549</td>\n      <td>@EBBerger (What shocked me in this thread is t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24637</th>\n      <td>33945</td>\n      <td>L'ÃÂ©pidÃÂ©mie de #coronavirus va accÃÂ©lÃ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7942</th>\n      <td>13451</td>\n      <td>#coronavirus\\r\\r\\nTearful nurse urges the publ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8834</th>\n      <td>14533</td>\n      <td>#Coronavirus fatalities are rising in #Europe,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27970</th>\n      <td>38097</td>\n      <td>I'll be posting regular updates on @OECD's pol...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24262</th>\n      <td>33467</td>\n      <td>The recent oil prices crash summed up in 1 twe...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3885</th>\n      <td>8514</td>\n      <td>$MAJOR #Major Cineplex Group PCL Major Cineple...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11543</th>\n      <td>17795</td>\n      <td>Our local french supermarket Fully stocked no ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15184</th>\n      <td>22283</td>\n      <td>Covid-19: Blackstone buys logistics assets as ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15120</th>\n      <td>22190</td>\n      <td>$AF #Air France-KLM ... ODDO : COVID-19: our t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1775</th>\n      <td>5961</td>\n      <td>$PDX #Paradox Interactive AB Paradox Interacti...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>523</th>\n      <td>4428</td>\n      <td>Covid19 panic. Pascal Montagne for @37degres a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1911</th>\n      <td>6127</td>\n      <td>#Coronavirus Fears May Drive U.S. #eCommerce S...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31838</th>\n      <td>42965</td>\n      <td>Chocolatiers slash prices as shoppers in #coro...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5815</th>\n      <td>10864</td>\n      <td>5 The world Don t panic and store food Idlib P...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27728</th>\n      <td>37786</td>\n      <td>Your patient with hypertension asks you for a ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23228</th>\n      <td>32182</td>\n      <td>One example of the complete IRRATIONALITY of t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28405</th>\n      <td>38640</td>\n      <td>Motilal Oswal - MOSL: Morning India (8/April/2...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26013</th>\n      <td>35666</td>\n      <td>To counterbalance the loss of revenue with the...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>32199</th>\n      <td>43405</td>\n      <td>ÃÂ«ÃÂ Industrial real-estate operators expec...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>966</th>\n      <td>4964</td>\n      <td>@DrAdrianHeald Did my weekly shop in local Fre...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12145</th>\n      <td>18527</td>\n      <td>SPOTTED Angela Merkel shopping in local superm...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31411</th>\n      <td>42446</td>\n      <td>The shift in demand has been a problem for fis...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27933</th>\n      <td>38046</td>\n      <td>Every department store in America has tried so...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Из-за разхождений с автором, я вручную найду твит, о котором идет речь, чтобы использовать правильный индекс\n",
    "\n",
    "train[train[\"Location\"].apply(lambda x: True if 'France' in x else False)].loc[train[\"Sentiment\"] == 0][[\"UserName\", \"OriginalTweet\", \"Sentiment\"]]\n",
    "# train.loc[train[\"UserName\"] == 17795]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T18:58:48.575789900Z",
     "start_time": "2025-05-11T18:58:48.560837300Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "aJVjjfqOJh8m",
    "ExecuteTime": {
     "end_time": "2025-05-11T19:13:59.661287300Z",
     "start_time": "2025-05-11T19:13:59.655531600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('Our local french supermarket Fully stocked no panic buying public calm and in lock down people around here respecting and looking out for each other Macron on the ball Johnson s handling of a complete cockup',\n np.int64(0))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 11656\n",
    "train.loc[ind]['OriginalTweet'], train.loc[ind]['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBMIHBI5KdaS"
   },
   "source": [
    "Автор твитта не доволен ситуацией с едой во Франции и текст имеет резко негативную окраску.\n",
    "\n",
    "Примените обученный CountVectorizer для векторизации данного текста, и попытайтесь определить самый важный токен и самый неважный токен (токен, компонента которого в векторе максимальна/минимальна, без учета 0). Хорошо ли они определились, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "7NcAllaEKsJj",
    "ExecuteTime": {
     "end_time": "2025-05-11T19:14:03.237073400Z",
     "start_time": "2025-05-11T19:14:03.194436200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: Our local french supermarket Fully stocked no panic buying public calm and in lock down people around here respecting and looking out for each other Macron on the ball Johnson s handling of a complete cockup\n",
      "Слово: around, Количество: 1\n",
      "Слово: ball, Количество: 1\n",
      "Слово: buying, Количество: 1\n",
      "Слово: calm, Количество: 1\n",
      "Слово: cockup, Количество: 1\n",
      "Слово: complete, Количество: 1\n",
      "Слово: french, Количество: 1\n",
      "Слово: fully, Количество: 1\n",
      "Слово: handling, Количество: 1\n",
      "Слово: johnson, Количество: 1\n",
      "Слово: local, Количество: 1\n",
      "Слово: lock, Количество: 1\n",
      "Слово: looking, Количество: 1\n",
      "Слово: macron, Количество: 1\n",
      "Слово: panic, Количество: 1\n",
      "Слово: people, Количество: 1\n",
      "Слово: public, Количество: 1\n",
      "Слово: respecting, Количество: 1\n",
      "Слово: stocked, Количество: 1\n",
      "Слово: supermarket, Количество: 1\n"
     ]
    }
   ],
   "source": [
    "X = cv.transform([train.loc[ind]['OriginalTweet']])\n",
    "\n",
    "feature_names = cv.get_feature_names_out()\n",
    "print(\"Текст:\", train.loc[ind]['OriginalTweet'])\n",
    "for i, value in enumerate(X.toarray()[0]):\n",
    "    if value > 0:\n",
    "        print(f\"Слово: {feature_names[i]}, Количество: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpEsl1k_NF4T"
   },
   "source": [
    "**Ответ:** Важность токенов при использовании count vectorizer невозможно понять, т.к. для маленьких текстов (одного из всего корпуса) токен либо встречается, либо нет, они почти всегда будут равнозначны (1), потому что чаще всего будет получаться так, что слово в маленьком предложении встретилось, либо нет, редко будут повторы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4DsEQpLO3J6"
   },
   "source": [
    "Теперь примените TfidfVectorizer и определите самый важный/неважный токены. Хорошо ли определились, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T19:41:09.584649200Z",
     "start_time": "2025-05-11T19:40:38.617042200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = df[\"OriginalTweet\"].array.tolist()\n",
    "# print(corpus[:10])\n",
    "tfidf = TfidfVectorizer(tokenizer=custom_tokenizer) \n",
    "tfidf.fit(corpus)\n",
    "\n",
    "print(len(tfidf.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: Our local french supermarket Fully stocked no panic buying public calm and in lock down people around here respecting and looking out for each other Macron on the ball Johnson s handling of a complete cockup\n",
      "Слово: cockup, Коэффициент: 0.3553802079275522\n",
      "Слово: macron, Коэффициент: 0.32501690193394317\n",
      "Слово: respecting, Коэффициент: 0.2933539314634002\n",
      "Слово: ball, Коэффициент: 0.28861197069455324\n",
      "Слово: french, Коэффициент: 0.2586934659627333\n",
      "Слово: johnson, Коэффициент: 0.2578097550200147\n",
      "Слово: handling, Коэффициент: 0.24616541420938234\n",
      "Слово: fully, Коэффициент: 0.22998277259882297\n",
      "Слово: complete, Коэффициент: 0.22887192456697944\n",
      "Слово: lock, Коэффициент: 0.2247672372164676\n",
      "Слово: calm, Коэффициент: 0.21679879075889003\n",
      "Слово: stocked, Коэффициент: 0.1964611544527103\n",
      "Слово: looking, Коэффициент: 0.1848066721893724\n",
      "Слово: public, Коэффициент: 0.16659450183277746\n",
      "Слово: around, Коэффициент: 0.16637284751832768\n",
      "Слово: local, Коэффициент: 0.14545427063152022\n",
      "Слово: buying, Коэффициент: 0.13603164035611906\n",
      "Слово: panic, Коэффициент: 0.12248531633586608\n",
      "Слово: people, Коэффициент: 0.10121917046777257\n",
      "Слово: supermarket, Коэффициент: 0.0940202275241061\n"
     ]
    }
   ],
   "source": [
    "X = tfidf.transform([train.loc[ind]['OriginalTweet']])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(\"Текст:\", train.loc[ind]['OriginalTweet'])\n",
    "coefs_sorted = {}\n",
    "for i, value in enumerate(X.toarray()[0]):\n",
    "    if value > 0:\n",
    "        coefs_sorted[feature_names[i]] = value\n",
    "        # print(f\"Слово: {feature_names[i]}, Коэффициент: {value}\")\n",
    "coefs_sorted = dict(sorted(coefs_sorted.items(), key=lambda x: -x[1]))\n",
    "for i in coefs_sorted.keys():\n",
    "    print(f\"Слово: {i}, Коэффициент: {coefs_sorted[i]}\")"
   ],
   "metadata": {
    "id": "uSNzdK3ENGB3",
    "ExecuteTime": {
     "end_time": "2025-05-11T19:42:15.124111200Z",
     "start_time": "2025-05-11T19:42:15.087501100Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYao_UhqQADm"
   },
   "source": [
    "**Ответ:** Теперь каждому токену вполне можно поставить меру важности - число, которое определял новый токенайзер. Он тем больше, чем чаще встречался токен в обучающем тексте и встречался в нашем тестируемом тексте. Сейчас можно сказать, что самые важные токены - cockup, macron и respecting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGRJPqfWSesQ"
   },
   "source": [
    "Найдите какой-нибудь положительно окрашенный твитт, где TfidfVectorizer хорошо (полезно для определения окраски) выделяет важный токен, поясните пример.\n",
    "\n",
    "*Подсказка:* явно положительные твитты можно искать при помощи положительных слов (good, great, amazing и т. д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "bRbQ2CHiSuJI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "outputId": "c4b34a7d-1076-4e1e-ad5c-9466fd2097c2",
    "ExecuteTime": {
     "end_time": "2025-05-11T20:34:09.116553100Z",
     "start_time": "2025-05-11T20:34:09.075228400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       UserName  ScreenName                       Location     TweetAt  \\\n14334     21202       66154                      Ohio, USA  23-03-2020   \n18296     26062       71014                 Somerville, MA  25-03-2020   \n15010     22046       66998                     Pittsburgh  23-03-2020   \n24347     33577       78529                        Unknown  05-04-2020   \n27273     37248       82200                   Brooklyn, NY  08-04-2020   \n17124     24641       69593                      Miami, FL  25-03-2020   \n11469     17705       62657                Zurich - Dublin  21-03-2020   \n25229     34701       79653                           Bury  06-04-2020   \n24155     33326       78278                   Langtoun, UK  05-04-2020   \n8740      14419       59371                         Nevada  20-03-2020   \n29159     39582       84534                        Alabama  09-04-2020   \n19660     27738       72690                        Unknown  26-03-2020   \n25447     34966       79918                          India  06-04-2020   \n31279     42276       87228         DÃÂ¼sseldorf, Germany  11-04-2020   \n19273     27258       72210  3rd planet in minor solar sys  26-03-2020   \n13355     20006       64958                Los Angeles, CA  22-03-2020   \n30061     40719       85671                        Unknown  10-04-2020   \n\n                                           OriginalTweet  Sentiment  \n14334  @drsmadden A5: I think brands are doing an exc...          1  \n18296  @UMassBoston @UMB_UR_BEST doctoral students wr...          1  \n15010  A massive thank you to @AldiUK ????\\r\\r\\n\\r\\r\\...          1  \n24347  Massive thanks to @waitrose for my delivery of...          1  \n27273  I m currently furloughed from my grocery store...          1  \n17124  So just CNN Or how about the Daily Mail that h...          1  \n11469  we ve put together a list of excellent recipe ...          1  \n25229  Solid Sense range, a name which illustrates @L...          1  \n24155  Local supermarket has a pre-opening hour exclu...          1  \n8740   My betrothed is a type-1 diabetic and manager ...          1  \n29159  This is an excellent idea , this stuff is rare...          1  \n19660  Harris Farm introduces excellent measures to f...          1  \n25447  Our ALL internal #auditor #Training  #course #...          1  \n31279  Pro tip: wonder what to do with all the #toile...          1  \n19273  Talked to the excellent about about transmissi...          1  \n13355  \"This particular edition of Writer's Lounge is...          1  \n30061  The @ftc has compiled an excellent list of res...          1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14334</th>\n      <td>21202</td>\n      <td>66154</td>\n      <td>Ohio, USA</td>\n      <td>23-03-2020</td>\n      <td>@drsmadden A5: I think brands are doing an exc...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18296</th>\n      <td>26062</td>\n      <td>71014</td>\n      <td>Somerville, MA</td>\n      <td>25-03-2020</td>\n      <td>@UMassBoston @UMB_UR_BEST doctoral students wr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15010</th>\n      <td>22046</td>\n      <td>66998</td>\n      <td>Pittsburgh</td>\n      <td>23-03-2020</td>\n      <td>A massive thank you to @AldiUK ????\\r\\r\\n\\r\\r\\...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24347</th>\n      <td>33577</td>\n      <td>78529</td>\n      <td>Unknown</td>\n      <td>05-04-2020</td>\n      <td>Massive thanks to @waitrose for my delivery of...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27273</th>\n      <td>37248</td>\n      <td>82200</td>\n      <td>Brooklyn, NY</td>\n      <td>08-04-2020</td>\n      <td>I m currently furloughed from my grocery store...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17124</th>\n      <td>24641</td>\n      <td>69593</td>\n      <td>Miami, FL</td>\n      <td>25-03-2020</td>\n      <td>So just CNN Or how about the Daily Mail that h...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11469</th>\n      <td>17705</td>\n      <td>62657</td>\n      <td>Zurich - Dublin</td>\n      <td>21-03-2020</td>\n      <td>we ve put together a list of excellent recipe ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25229</th>\n      <td>34701</td>\n      <td>79653</td>\n      <td>Bury</td>\n      <td>06-04-2020</td>\n      <td>Solid Sense range, a name which illustrates @L...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24155</th>\n      <td>33326</td>\n      <td>78278</td>\n      <td>Langtoun, UK</td>\n      <td>05-04-2020</td>\n      <td>Local supermarket has a pre-opening hour exclu...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8740</th>\n      <td>14419</td>\n      <td>59371</td>\n      <td>Nevada</td>\n      <td>20-03-2020</td>\n      <td>My betrothed is a type-1 diabetic and manager ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29159</th>\n      <td>39582</td>\n      <td>84534</td>\n      <td>Alabama</td>\n      <td>09-04-2020</td>\n      <td>This is an excellent idea , this stuff is rare...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19660</th>\n      <td>27738</td>\n      <td>72690</td>\n      <td>Unknown</td>\n      <td>26-03-2020</td>\n      <td>Harris Farm introduces excellent measures to f...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25447</th>\n      <td>34966</td>\n      <td>79918</td>\n      <td>India</td>\n      <td>06-04-2020</td>\n      <td>Our ALL internal #auditor #Training  #course #...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>31279</th>\n      <td>42276</td>\n      <td>87228</td>\n      <td>DÃÂ¼sseldorf, Germany</td>\n      <td>11-04-2020</td>\n      <td>Pro tip: wonder what to do with all the #toile...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19273</th>\n      <td>27258</td>\n      <td>72210</td>\n      <td>3rd planet in minor solar sys</td>\n      <td>26-03-2020</td>\n      <td>Talked to the excellent about about transmissi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13355</th>\n      <td>20006</td>\n      <td>64958</td>\n      <td>Los Angeles, CA</td>\n      <td>22-03-2020</td>\n      <td>\"This particular edition of Writer's Lounge is...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30061</th>\n      <td>40719</td>\n      <td>85671</td>\n      <td>Unknown</td>\n      <td>10-04-2020</td>\n      <td>The @ftc has compiled an excellent list of res...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['OriginalTweet'].apply(lambda x: 'excellent' in x) & (train['Sentiment'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ind = 15010\n",
    "\n",
    "X = tfidf.transform([train.loc[ind]['OriginalTweet']])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(\"Текст:\", train.loc[ind]['OriginalTweet'])\n",
    "coefs_sorted = {}\n",
    "for i, value in enumerate(X.toarray()[0]):\n",
    "    if value > 0:\n",
    "        coefs_sorted[feature_names[i]] = value\n",
    "        # print(f\"Слово: {feature_names[i]}, Коэффициент: {value}\")\n",
    "coefs_sorted = dict(sorted(coefs_sorted.items(), key=lambda x: -x[1]))\n",
    "for i in coefs_sorted.keys():\n",
    "    print(f\"Слово: {i}, Коэффициент: {coefs_sorted[i]}\")"
   ],
   "metadata": {
    "id": "jSjbKPCWk87K",
    "ExecuteTime": {
     "end_time": "2025-05-11T20:35:30.320117100Z",
     "start_time": "2025-05-11T20:35:30.253811400Z"
    }
   },
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: A massive thank you to @AldiUK ????\r\n",
      "\r\n",
      "Thanks for sticking to the 4 items limit. And your excellent staff.\r\n",
      "\r\n",
      "In your own way. You are saving lives too.\r\n",
      "\r\n",
      "This is why I love #Aldi\r\n",
      "On both sides of the Atlantic. Not just for the prices.\r\n",
      "#StayAtHomeSaveLives #COVID?19 #coronavirus\n",
      "Слово: atlantic, Коэффициент: 0.3111424035809931\n",
      "Слово: sides, Коэффициент: 0.3072179605530729\n",
      "Слово: sticking, Коэффициент: 0.30053176243187807\n",
      "Слово: #aldi, Коэффициент: 0.2901976312119283\n",
      "Слово: #stayathomesavelives, Коэффициент: 0.2745374603255903\n",
      "Слово: excellent, Коэффициент: 0.26011475148354124\n",
      "Слово: @aldiuk, Коэффициент: 0.25144231249796184\n",
      "Слово: saving, Коэффициент: 0.24345659152602794\n",
      "Слово: massive, Коэффициент: 0.21649774682265552\n",
      "Слово: limit, Коэффициент: 0.2059535007574295\n",
      "Слово: love, Коэффициент: 0.19795616016716008\n",
      "Слово: lives, Коэффициент: 0.19049331266360928\n",
      "Слово: thanks, Коэффициент: 0.17777284276642807\n",
      "Слово: 4, Коэффициент: 0.1743766117275559\n",
      "Слово: items, Коэффициент: 0.16210316431682412\n",
      "Слово: way, Коэффициент: 0.15927935262897688\n",
      "Слово: thank, Коэффициент: 0.15708261140030916\n",
      "Слово: staff, Коэффициент: 0.15471575234469567\n",
      "Слово: #covid, Коэффициент: 0.15463968095265737\n",
      "Слово: prices, Коэффициент: 0.08927571248072623\n",
      "Слово: 19, Коэффициент: 0.07393829913565325\n",
      "Слово: #coronavirus, Коэффициент: 0.06607655440700544\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTv9ST2_U6NA"
   },
   "source": [
    "**Ответ:** Видим в топе среди обычных нейтральных слов или тегов еще положительно окрашенные слова - love, thanks, excellent. Так как они в вершине топа, то они прямо намекают на нужный стиль твита."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVEuZm8BHms6"
   },
   "source": [
    "## Задание 4 Обучение первых моделей (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JADkO3sfXdOG"
   },
   "source": [
    "Примените оба векторайзера для получения матриц с признаками текстов.  Выделите целевую переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DguoiXhCX2oN"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FX1KSOfYSx4"
   },
   "source": [
    "Обучите логистическую регрессию на векторах из обоих векторайзеров. Посчитайте долю правильных ответов на обучающих и тестовых данных. Какой векторайзер показал лучший результат? Что можно сказать о моделях?\n",
    "\n",
    "Используйте `sparse` матрицы (после векторизации), не превращайте их в `numpy.ndarray` или `pd.DataFrame` - может не хватить памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Tb3eh8UXJ6v"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8y_wO7rCmv7K"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSOR1i3mjrys"
   },
   "source": [
    "## Задание 5 Стемминг (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6ONBWNPjuq-"
   },
   "source": [
    "Для уменьшения словаря можно использовать стемминг.\n",
    "\n",
    "Модифицируйте написанный токенайзер, добавив в него стемминг с использованием SnowballStemmer. Обучите Count- и Tfidf- векторайзеры. Как изменился размер словаря?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVfA2-iMkQBb"
   },
   "outputs": [],
   "source": [
    "def custom_stem_tokenizer(text):\n",
    "    # -- YOUR CODE HERE --\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QmrjYtqnlPd",
    "outputId": "cd91291d-9676-4611-9fc4-28afaed58963"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['sampl', 'text', '@sample_text', '#sampletext', 'ad', 'word', 'check', 'stem']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "custom_stem_tokenizer(\n",
    "    'This is sample text!!!! @Sample_text I, \\x92\\x92 https://t.co/sample  #sampletext adding more words to check stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAvUTmaplzOS",
    "outputId": "566207fe-183b-4ed6-d333-f86f0cc9ae38"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "36652\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer  # -- YOUR CODE HERE --\n",
    "\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oyzs5TaAoHP6"
   },
   "source": [
    "**Ответ** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OkncHI8oRmd"
   },
   "source": [
    "Обучите логистическую регрессию с использованием обоих векторайзеров. Изменилось ли качество? Есть ли смысл применять стемминг?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykZJPphEoZ5W"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCRlrODro0h8"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYWGQNEDqLC-"
   },
   "source": [
    "## Задание  6 Работа с частотами (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hq-tl5mqUSn"
   },
   "source": [
    "Еще один способ уменьшить количество признаков - это использовать параметры min_df и max_df при построении векторайзера  эти параметры помогают ограничить требуемую частоту встречаемости токена в документах.\n",
    "\n",
    "По умолчанию берутся все токены, которые встретились хотя бы один раз.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1SiD4DE3WZ2"
   },
   "source": [
    "Подберите max_df такой, что размер словаря будет 36651 (на 1 меньше, чем было). Почему параметр получился такой большой/маленький?"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cv_df = CountVectorizer(tokenizer=custom_stem_tokenizer,\n",
    "                        max_df=  # -- YOUR CODE HERE --\n",
    "                        ).fit(\n",
    "    # -- YOUR CODE HERE --\n",
    ")\n",
    "print(len(cv_df.vocabulary_))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3YLb8PViExb",
    "outputId": "b6d67654-d232-4e11-a5ca-6f2145053e98"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "36651\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "tyEpkJUkjnuK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdZYoGZR4UsA"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gRIUaB1u32f"
   },
   "source": [
    "Подберите min_df (используйте дефолтное значение max_df) в CountVectorizer таким образом, чтобы размер словаря был 3700 токенов (при использовании токенайзера со стеммингом), а качество осталось таким же, как и было. Что можно сказать о результатах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSnMJkn9XmsT",
    "outputId": "e0d8eb21-e5d7-46b4-e1d1-4b1ae220e9a0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3700\n"
     ]
    }
   ],
   "source": [
    "cv_df = CountVectorizer(tokenizer=custom_stem_tokenizer,\n",
    "                        min_df=  # -- YOUR CODE HERE --\n",
    "                        ).fit(\n",
    "    # -- YOUR CODE HERE --\n",
    ")\n",
    "print(len(cv_df.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "mvMDwpdfjm8Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fGYpUIZx0fk"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "В предыдущих заданиях признаки не скалировались. Отскалируйте данные (при словаре размера 3.7 тысяч, векторизованные CountVectorizer), обучите логистическую регрессию, посмотрите качество и выведите `barplot`, содержащий по 10 токенов, с наибольшим по модулю положительными/отрицательными весами. Что можно сказать об этих токенах?"
   ],
   "metadata": {
    "id": "Gx_h_-inKbBl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "KBATXJX6LG9q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ],
   "metadata": {
    "id": "ThcEfzY1LHET"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktJVOdrIHq7B"
   },
   "source": [
    "## Задание 7 Другие признаки (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt3jRCZ2H0Og"
   },
   "source": [
    "Мы были сконцентрированы на работе с текстами твиттов и не использовали другие признаки - имена пользователя, дату и местоположение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52wjewCCo_di"
   },
   "source": [
    "Изучите признаки UserName и ScreenName. полезны ли они? Если полезны, то закодируйте их, добавьте к матрице с отскалированными признаками, обучите логистическую регрессию, замерьте качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63thouYZptj6"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8_qR-gnpT3a"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ythEcFSkt7y3"
   },
   "source": [
    "Изучите признак TweetAt в обучающей выборке: преобразуйте его к типу datetime и нарисуйте его гистограмму с разделением по цвету на основе целевой переменной. Полезен ли он? Если полезен, то закодируйте его, добавьте к матрице с отскалированными признаками, обучите логистическую регрессию, замерьте качество."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "Lxb_k0JLirNv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IdLBdpQxM-G"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поработайте с признаком Location в обучающей выборке. Сколько уникальных значений?"
   ],
   "metadata": {
    "id": "r2JtRPhNP6qx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "xYQZQ1FRNpoe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Постройте гистограмму топ-10 по популярности местоположений (исключая Unknown)"
   ],
   "metadata": {
    "id": "6k4JwpRTQISa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "J91YkhegJ0mz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видно, что многие местоположения включают в себя более точное название места, чем другие (Например, у некоторых стоит London, UK; а у некоторых просто UK или United Kingdom).\n",
    "\n",
    "Создайте новый признак WiderLocation, который содержит самое широкое местоположение (например, из London, UK должно получиться UK). Сколько уникальных категорий теперь? Постройте аналогичную гистограмму."
   ],
   "metadata": {
    "id": "ZOsv3lODTfYB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "mSkow6acOMyD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Закодируйте признак WiderLocation с помощью OHE таким образом, чтобы создались только столбцы для местоположений, которые встречаются более одного раза. Сколько таких значений?\n"
   ],
   "metadata": {
    "id": "cgyWrD2eVfff"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "SeJBfBWgPvg_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Добавьте этот признак к матрице отскалированных текстовых признаков, обучите логистическую регрессию, замерьте качество. Как оно изменилось? Оказался ли признак полезным?\n",
    "\n",
    "\n",
    "*Подсказка:* используйте параметр `categories` в энкодере."
   ],
   "metadata": {
    "id": "ZyMX5kZuimPK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "EO1jNPeeim7A"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ],
   "metadata": {
    "id": "7dHsGlDRYUQt"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
