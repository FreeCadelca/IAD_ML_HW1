{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffq6A2-ifzAA"
   },
   "source": [
    "# Интеллектуальный анализ данных – весна 2025\n",
    "# Домашнее задание 6: классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Правила:\n",
    "\n",
    "\n",
    "\n",
    "*   Домашнее задание оценивается в 10 баллов.\n",
    "*   Можно использовать без доказательства любые результаты, встречавшиеся на лекциях или семинарах по курсу, если получение этих результатов не является вопросом задания.\n",
    "*  Можно использовать любые свободные источники с *обязательным* указанием ссылки на них.\n",
    "*  Плагиат не допускается. При обнаружении случаев списывания, 0 за работу выставляется всем участникам нарушения, даже если можно установить, кто у кого списал.\n",
    "*  Старайтесь сделать код как можно более оптимальным. В частности, будет штрафоваться использование циклов в тех случаях, когда операцию можно совершить при помощи инструментов библиотек, о которых рассказывалось в курсе.\n",
    "* Если в задании есть вопрос на рассуждение, то за отсутствие ответа на него балл за задание будет снижен вполовину."
   ],
   "metadata": {
    "id": "EPcxtekTA1Sm"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itRtFtrOf0_b"
   },
   "source": [
    "В этом домашнем задании вам предстоит построить классификатор текстов.\n",
    "\n",
    "Будем предсказывать эмоциональную окраску твиттов о коронавирусе.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tNGRVO7_g9mz",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:00.675448600Z",
     "start_time": "2025-05-12T15:27:00.670419100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zOy8iHJQg_Ss",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "outputId": "6a32c325-1b9a-4895-ab22-5e985016da91",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:00.791678700Z",
     "start_time": "2025-05-12T15:27:00.673684200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       UserName  ScreenName         Location     TweetAt  \\\n22335     31087       76039   Des Moines, IA  03-04-2020   \n18236     25987       70939    Cleveland, OH  25-03-2020   \n27527     37552       82504         Brussels  08-04-2020   \n1361       5449       50401  Toronto, Canada  17-03-2020   \n\n                                           OriginalTweet           Sentiment  \n22335  We greatly appreciate the @DeltaDentalIA Found...  Extremely Negative  \n18236  DIY Moisturizing Hand Sanitizer \\r\\r\\n1. Aloe ...            Positive  \n27527  How are consumer trends faring in the #coronav...            Positive  \n1361   Not going to name names, but when this is all ...            Negative  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22335</th>\n      <td>31087</td>\n      <td>76039</td>\n      <td>Des Moines, IA</td>\n      <td>03-04-2020</td>\n      <td>We greatly appreciate the @DeltaDentalIA Found...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>18236</th>\n      <td>25987</td>\n      <td>70939</td>\n      <td>Cleveland, OH</td>\n      <td>25-03-2020</td>\n      <td>DIY Moisturizing Hand Sanitizer \\r\\r\\n1. Aloe ...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>27527</th>\n      <td>37552</td>\n      <td>82504</td>\n      <td>Brussels</td>\n      <td>08-04-2020</td>\n      <td>How are consumer trends faring in the #coronav...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>1361</th>\n      <td>5449</td>\n      <td>50401</td>\n      <td>Toronto, Canada</td>\n      <td>17-03-2020</td>\n      <td>Not going to name names, but when this is all ...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_coronavirus.csv', encoding='latin-1')\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2OiDog9ZBlS"
   },
   "source": [
    "Для каждого твитта указано:\n",
    "\n",
    "\n",
    "*   UserName - имя пользователя, заменено на целое число для анонимности\n",
    "*   ScreenName - отображающееся имя пользователя, заменено на целое число для анонимности\n",
    "*   Location - местоположение\n",
    "*   TweetAt - дата создания твитта\n",
    "*   OriginalTweet - текст твитта\n",
    "*   Sentiment - эмоциональная окраска твитта (целевая переменная)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZTMseDkhTC7"
   },
   "source": [
    "## Задание 1 Подготовка (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx2-odn9hdAW"
   },
   "source": [
    "Целевая переменная находится в колонке `Sentiment`.  Преобразуйте ее таким образом, чтобы она стала бинарной: 1 - если у твитта положительная или очень положительная эмоциональная окраска и 0 - если отрицательная или очень отрицательная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZaQKQ1zEjP15",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:00.807082700Z",
     "start_time": "2025-05-12T15:27:00.794663200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       UserName  ScreenName           Location     TweetAt  \\\n19505     27545       72497  Glasgow, Scotland  26-03-2020   \n11584     17848       62800           Missouri  21-03-2020   \n16082     23371       68323   Hyderabad, India  24-03-2020   \n12965     19519       64471              Kenya  22-03-2020   \n\n                                           OriginalTweet  Sentiment  \n19505  If you are shopping online with Topshop or Top...          0  \n11584  It seems that our most vulnerable age group ar...          0  \n16082  @PMOIndia @narendramodi dear sir , my humble r...          0  \n12965  With the the Covid-19 epidemic! Heri Online wi...          1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19505</th>\n      <td>27545</td>\n      <td>72497</td>\n      <td>Glasgow, Scotland</td>\n      <td>26-03-2020</td>\n      <td>If you are shopping online with Topshop or Top...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11584</th>\n      <td>17848</td>\n      <td>62800</td>\n      <td>Missouri</td>\n      <td>21-03-2020</td>\n      <td>It seems that our most vulnerable age group ar...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16082</th>\n      <td>23371</td>\n      <td>68323</td>\n      <td>Hyderabad, India</td>\n      <td>24-03-2020</td>\n      <td>@PMOIndia @narendramodi dear sir , my humble r...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12965</th>\n      <td>19519</td>\n      <td>64471</td>\n      <td>Kenya</td>\n      <td>22-03-2020</td>\n      <td>With the the Covid-19 epidemic! Heri Online wi...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'] = df['Sentiment'].apply(lambda x: 1 if x in ('Positive', 'Extremely Positive') else 0)\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGq1FxJ-kBo5"
   },
   "source": [
    "Сбалансированы ли классы?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a7gdNtxckK5V",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:00.813596Z",
     "start_time": "2025-05-12T15:27:00.805083100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "1    18046\n",
      "0    15398\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng8BCelMkWb0"
   },
   "source": [
    "**Ответ:** Классы сбалансированы достаточно, но не идеально"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmSIBSsLk5Zz"
   },
   "source": [
    "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их строкой 'Unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UhUVRkR5kxa7",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:00.878234800Z",
     "start_time": "2025-05-12T15:27:00.814720100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data in columns before fill:\n",
      "UserName 0\n",
      "ScreenName 0\n",
      "Location 7049\n",
      "TweetAt 0\n",
      "OriginalTweet 0\n",
      "Sentiment 0\n",
      "Missing data in columns after fill:\n",
      "UserName 0\n",
      "ScreenName 0\n",
      "Location 0\n",
      "TweetAt 0\n",
      "OriginalTweet 0\n",
      "Sentiment 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing data in columns before fill:\")\n",
    "for i in df.head():\n",
    "    print(i, df[i].isna().sum())\n",
    "for i in df.head():\n",
    "    df[i].fillna(\"Unknown\", inplace=True)\n",
    "print(\"Missing data in columns after fill:\")\n",
    "for i in df.head():\n",
    "    print(i, df[i].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tzt27tfjUpq"
   },
   "source": [
    "Разделите данные на обучающие и тестовые в соотношении 7 : 3 и укажите `random_state=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xSLOA9tIj9Z6",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:01.650811300Z",
     "start_time": "2025-05-12T15:27:00.841667900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.3, stratify=df['Sentiment'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9RrPUsJlL60"
   },
   "source": [
    "## Задание 2 Токенизация (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Dz_b7Xopc_R"
   },
   "source": [
    "Постройте словарь на основе обучающей выборки и посчитайте количество встреч каждого токена с использованием самой простой токенизации - деления текстов по пробельным символам и приведения токенов в нижний регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SFr67WOJphny",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:02.002624500Z",
     "start_time": "2025-05-12T15:27:01.623810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    advice Talk to your neighbours family to excha...\n",
      "1    Coronavirus Australia: Woolworths to give elde...\n",
      "2    My food stock is not the only one which is emp...\n",
      "3    Me, ready to go at supermarket during the #COV...\n",
      "4    As news of the regionÃÂs first confirmed COV...\n",
      "Name: OriginalTweet, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": "              Word  Count\n0              the  38250\n1               to  33447\n2              and  20935\n3               of  18578\n4                a  16667\n...            ...    ...\n103195  @tartiicat      1\n103196    new/used      1\n103197        rift      1\n103198     $700.00      1\n103199      whethe      1\n\n[103200 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the</td>\n      <td>38250</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>to</td>\n      <td>33447</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>and</td>\n      <td>20935</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>of</td>\n      <td>18578</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a</td>\n      <td>16667</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>103195</th>\n      <td>@tartiicat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103196</th>\n      <td>new/used</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103197</th>\n      <td>rift</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103198</th>\n      <td>$700.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103199</th>\n      <td>whethe</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>103200 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "print(df[\"OriginalTweet\"].head(5))\n",
    "\n",
    "for i in df[\"OriginalTweet\"]:\n",
    "    for word in i.split():\n",
    "        words.append(word.lower())\n",
    "\n",
    "words = Counter(words)\n",
    "words_count = words.most_common()\n",
    "words_count_df = pd.DataFrame(words_count, columns=['Word', 'Count'])\n",
    "\n",
    "words_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe0h2Jqkpnao"
   },
   "source": [
    "Какой размер словаря получился?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:** Как мы видим, получилось 103200 строк, значит столько же различных токенов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d2G1Z-Qpqkd"
   },
   "source": [
    "Выведите 10 самых популярных токенов с количеством встреч каждого из них. Объясните, почему именно эти токены в топе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Impi32a_pssg",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:02.007589700Z",
     "start_time": "2025-05-12T15:27:01.998486600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           Word  Count\n0           the  38250\n1            to  33447\n2           and  20935\n3            of  18578\n4             a  16667\n5            in  16024\n6           for  12193\n7  #coronavirus  11759\n8            is  10596\n9           are   9958",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the</td>\n      <td>38250</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>to</td>\n      <td>33447</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>and</td>\n      <td>20935</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>of</td>\n      <td>18578</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a</td>\n      <td>16667</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>in</td>\n      <td>16024</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>for</td>\n      <td>12193</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#coronavirus</td>\n      <td>11759</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>is</td>\n      <td>10596</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>are</td>\n      <td>9958</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtuJCD0ApuFd"
   },
   "source": [
    "**Ответ:** Логично, что артикли, предлоги и ```#coronavirus``` оказались в топе, потому что артикли и предлоги - сами по себе частые связки в тексте, а хештег - просто тематика всех текстов, логично, что такой хештег будет "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7DTQDkWsVYp"
   },
   "source": [
    "Удалите стоп-слова из словаря и выведите новый топ-10 токенов (и количество встреч) по популярности.  Что можно сказать  о нем?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8csSAdgTsnFx",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:03.062084800Z",
     "start_time": "2025-05-12T15:27:02.003593900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            Word  Count\n7   #coronavirus  11759\n16        prices   5625\n18          food   5409\n23       grocery   4882\n24   supermarket   4662\n25      covid-19   4504\n26        people   4488\n27         store   4486\n35      #covid19   3561\n39      consumer   3233",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>#coronavirus</td>\n      <td>11759</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>prices</td>\n      <td>5625</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>food</td>\n      <td>5409</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>grocery</td>\n      <td>4882</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>supermarket</td>\n      <td>4662</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>covid-19</td>\n      <td>4504</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>people</td>\n      <td>4488</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>store</td>\n      <td>4486</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>#covid19</td>\n      <td>3561</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>consumer</td>\n      <td>3233</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stops = set(stopwords.words('english'))\n",
    "words_count_df_copy = words_count_df[~words_count_df[\"Word\"].isin(stops)]\n",
    "words_count_df_copy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZH0x2Lzs-Dh"
   },
   "source": [
    "**Ответ:** Видим, что теперь токены очистились от мусора - артиклей, предлогов и местоимений, которые, логично, есть в каждом тексте, и токены отражают обычные хештеги или самые частые слова. Я вижу, что многие из них связаны с едой или пищевой промышленностью. Возможно, это произошло, потому что еда - главное в выживании во времена коронавируса, а также супермаркеты - неизбежное скопление людей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKSGRyI-uor0"
   },
   "source": [
    "Также выведите 20 самых непопулярных слов (если самых непопулярных слов больше, выведите любые 20 из них) Почему эти токены непопулярны, требуется ли как-то дополнительно работать с ними?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "moArbwfvun9t",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:03.077728800Z",
     "start_time": "2025-05-12T15:27:03.064086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                           Word  Count\n103180                bullion's      1\n103181              #safe-haven      1\n103182          $1,715.25/ounce      1\n103183                     dec.      1\n103184                $1,722.20      1\n103185                rs.46,215      1\n103186                     gms.      1\n103187  https://t.co/s8coy5vvgn      1\n103188                   home??      1\n103189  https://t.co/v8xdxhqeyn      1\n103190           @mrsilverscott      1\n103191                  delays.      1\n103192                rejecting      1\n103193            @kameronwilds      1\n103194            martinsville,      1\n103195               @tartiicat      1\n103196                 new/used      1\n103197                     rift      1\n103198                  $700.00      1\n103199                   whethe      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>103180</th>\n      <td>bullion's</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103181</th>\n      <td>#safe-haven</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103182</th>\n      <td>$1,715.25/ounce</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103183</th>\n      <td>dec.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103184</th>\n      <td>$1,722.20</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103185</th>\n      <td>rs.46,215</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103186</th>\n      <td>gms.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103187</th>\n      <td>https://t.co/s8coy5vvgn</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103188</th>\n      <td>home??</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103189</th>\n      <td>https://t.co/v8xdxhqeyn</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103190</th>\n      <td>@mrsilverscott</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103191</th>\n      <td>delays.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103192</th>\n      <td>rejecting</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103193</th>\n      <td>@kameronwilds</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103194</th>\n      <td>martinsville,</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103195</th>\n      <td>@tartiicat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103196</th>\n      <td>new/used</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103197</th>\n      <td>rift</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103198</th>\n      <td>$700.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103199</th>\n      <td>whethe</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_df_copy.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRp3J1gQunlR"
   },
   "source": [
    "**Ответ:** Эти токены непопулярны, потому что содержат пунктуацию, ссылки, конкретные числа/цифры или непопулярные хештеги. Я считаю, что их нужно обработать, удалив пунктуацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wx9LQOSPzvjV"
   },
   "source": [
    "Теперь воспользуемся токенайзером получше - TweetTokenizer из библиотеки nltk. Примените его и посмотрите на топ-10 популярных слов. Чем он отличается от топа, который получался раньше? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2G1UkyVxzvFY",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:06.738717900Z",
     "start_time": "2025-05-12T15:27:03.070721200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               Word  Count\n0               the  34781\n1                 .  34284\n2                to  32812\n3                 ,  25142\n4               and  20439\n...             ...    ...\n87328  martinsville      1\n87329    @TartiiCat      1\n87330          Rift      1\n87331        700.00      1\n87332        whethe      1\n\n[87333 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the</td>\n      <td>34781</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>.</td>\n      <td>34284</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>to</td>\n      <td>32812</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>,</td>\n      <td>25142</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>and</td>\n      <td>20439</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>87328</th>\n      <td>martinsville</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87329</th>\n      <td>@TartiiCat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87330</th>\n      <td>Rift</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87331</th>\n      <td>700.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87332</th>\n      <td>whethe</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>87333 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "words = []\n",
    "\n",
    "for i in df[\"OriginalTweet\"]:\n",
    "    words += tokenizer.tokenize(i)\n",
    "\n",
    "words = Counter(words)\n",
    "words_count = words.most_common()\n",
    "words_count_df = pd.DataFrame(words_count, columns=['Word', 'Count'])\n",
    "\n",
    "words_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50eVUnJN1Zxl"
   },
   "source": [
    "**Ответ:** На удивление, сейчас токенайзер выдал даже более худшие результаты, чем \"рабоче-крестьянский\" метод с split, потому что наш новый токенайзер отделяет знаки пунктуации от основного текста, и, естественно, знаков пунктуации в топе много."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gqQgiMs11bs"
   },
   "source": [
    "Удалите из словаря стоп-слова и пунктуацию, посмотрите на новый топ-10 слов с количеством встреч, есть ли теперь в нем что-то не похожее на слова?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0yHWdFrp0Mup",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:06.760338700Z",
     "start_time": "2025-05-12T15:27:06.736187200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               Word  Count\n10                Â  10498\n11                  10361\n13     #coronavirus  10211\n14               19  10142\n18                I   7484\n...             ...    ...\n87328  martinsville      1\n87329    @TartiiCat      1\n87330          Rift      1\n87331        700.00      1\n87332        whethe      1\n\n[87110 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>Â</td>\n      <td>10498</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td></td>\n      <td>10361</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>#coronavirus</td>\n      <td>10211</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>19</td>\n      <td>10142</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>I</td>\n      <td>7484</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>87328</th>\n      <td>martinsville</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87329</th>\n      <td>@TartiiCat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87330</th>\n      <td>Rift</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87331</th>\n      <td>700.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87332</th>\n      <td>whethe</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>87110 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "punctuation_list = list(punctuation)\n",
    "words_count_df_copy = words_count_df[\n",
    "    ~words_count_df[\"Word\"].isin(punctuation_list) & ~words_count_df[\"Word\"].isin(stops)]\n",
    "words_count_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZJqXELP_Yxy"
   },
   "source": [
    "**Ответ:** Да, после очистки от мусора можно видеть а-ля адекватный список топа слов. Однако все еще присутствует мусор - нечитаемые символы, возможно, служебные, возможно, смайлики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzXjMsSB_kXB"
   },
   "source": [
    "Скорее всего в некоторых топах были неотображаемые символы или отдельные буквы не латинского алфавита. Уберем их: удалите из словаря токены из одного символа, позиция которого в таблице Unicode 128 и более (`ord(x) >= 128`)\n",
    "\n",
    "Выведите топ-10 самых популярных и топ-20 непопулярных слов. Чем полученные топы отличаются от итоговых топов, полученных при использовании токенизации по пробелам? Что теперь лучше, а что хуже?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1695hlkS_1-J",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:06.863682800Z",
     "start_time": "2025-05-12T15:27:06.753338800Z"
    }
   },
   "outputs": [],
   "source": [
    "words_count_df_copy = words_count_df_copy[\n",
    "    (words_count_df_copy[\"Word\"].str.len() > 1) |\n",
    "    (words_count_df_copy[\"Word\"].apply(\n",
    "        lambda x: ord(x) if len(x) == 1 else 0\n",
    "    ) <= 127)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "            Word  Count\n13  #coronavirus  10211\n14            19  10142\n18             I   7484\n23        prices   6166\n24         COVID   5945\n26          food   5423\n29         store   5234\n33   supermarket   4803\n35       grocery   4350\n36        people   4300",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td>#coronavirus</td>\n      <td>10211</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>19</td>\n      <td>10142</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>I</td>\n      <td>7484</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>prices</td>\n      <td>6166</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>COVID</td>\n      <td>5945</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>food</td>\n      <td>5423</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>store</td>\n      <td>5234</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>supermarket</td>\n      <td>4803</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>grocery</td>\n      <td>4350</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>people</td>\n      <td>4300</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_df_copy.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:06.866686200Z",
     "start_time": "2025-05-12T15:27:06.792745900Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                          Word  Count\n87313  https://t.co/qUQ8Y0uM6n      1\n87314             @MajangChien      1\n87315                bullion's      1\n87316              #safe-haven      1\n87317                    1,715      1\n87318                    1,722      1\n87319                   46,215      1\n87320                      gms      1\n87321  https://t.co/S8coY5VVgN      1\n87322  https://t.co/v8XDXhqeYN      1\n87323                 roulette      1\n87324           @MrSilverScott      1\n87325                rejecting      1\n87326                      TAT      1\n87327            @KameronWilds      1\n87328             martinsville      1\n87329               @TartiiCat      1\n87330                     Rift      1\n87331                   700.00      1\n87332                   whethe      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>87313</th>\n      <td>https://t.co/qUQ8Y0uM6n</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87314</th>\n      <td>@MajangChien</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87315</th>\n      <td>bullion's</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87316</th>\n      <td>#safe-haven</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87317</th>\n      <td>1,715</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87318</th>\n      <td>1,722</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87319</th>\n      <td>46,215</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87320</th>\n      <td>gms</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87321</th>\n      <td>https://t.co/S8coY5VVgN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87322</th>\n      <td>https://t.co/v8XDXhqeYN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87323</th>\n      <td>roulette</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87324</th>\n      <td>@MrSilverScott</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87325</th>\n      <td>rejecting</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87326</th>\n      <td>TAT</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87327</th>\n      <td>@KameronWilds</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87328</th>\n      <td>martinsville</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87329</th>\n      <td>@TartiiCat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87330</th>\n      <td>Rift</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87331</th>\n      <td>700.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>87332</th>\n      <td>whethe</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_df_copy.tail(20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:06.891879100Z",
     "start_time": "2025-05-12T15:27:06.797527700Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzjHAKIlDvc6"
   },
   "source": [
    "**Ответ:** Теперь топ похож на реальный, немного отличается от нашего прошлого способа (в том числе, потому что мы не приводили слова к нижнему регистру). Из плюсов: разделения стали лучше, потому что не учитывается пунктуация и маленькие различия между словами, из минусов: появились местоимения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcDf9_6HB2zm"
   },
   "source": [
    "Выведите топ-10 популярных хештегов (токены, первые символы которых - #) с количеством встреч. Что можно сказать о них?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zk4fygCUBw3l",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:07.017483500Z",
     "start_time": "2025-05-12T15:27:06.802800100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Temp\\ipykernel_2060\\533891431.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  words_count_df_copy[words_count_df[\"Word\"].str[0] == '#'].head(10)\n"
     ]
    },
    {
     "data": {
      "text/plain": "              Word  Count\n13    #coronavirus  10211\n60        #COVID19   2621\n75       #Covid_19   2126\n90    #Coronavirus   1806\n116     #COVID2019   1341\n159   #toiletpaper    944\n175       #covid19    829\n186         #COVID    775\n246  #CoronaCrisis    599\n284   #CoronaVirus    525",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td>#coronavirus</td>\n      <td>10211</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>#COVID19</td>\n      <td>2621</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>#Covid_19</td>\n      <td>2126</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>#Coronavirus</td>\n      <td>1806</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>#COVID2019</td>\n      <td>1341</td>\n    </tr>\n    <tr>\n      <th>159</th>\n      <td>#toiletpaper</td>\n      <td>944</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>#covid19</td>\n      <td>829</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>#COVID</td>\n      <td>775</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>#CoronaCrisis</td>\n      <td>599</td>\n    </tr>\n    <tr>\n      <th>284</th>\n      <td>#CoronaVirus</td>\n      <td>525</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_df_copy[words_count_df[\"Word\"].str[0] == '#'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6NeNWBkDxM7"
   },
   "source": [
    "**Ответ:** О хештегах можно сказать следующее: все хештеги, очевидно, упоминают эпидемию. Один забавный хештег смог затесаться в этот топ - #toiletpaper, все мы знаем, почему :))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLYBg7caD5GA"
   },
   "source": [
    "То же самое проделайте для ссылок на сайт https://t.co Сравнима ли популярность ссылок с популярностью хештегов? Будет ли информация о ссылке на конкретную страницу полезна?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MXbm1oeaCK9S",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:07.019483400Z",
     "start_time": "2025-05-12T15:27:06.838491200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Word  Count\n",
      "11014  https://t.co/oXA7SWtoNd      6\n",
      "12562  https://t.co/G63RP042HO      5\n",
      "13125  https://t.co/R7sAGojsjg      4\n",
      "13354  https://t.co/WrLHYzIzAA      4\n",
      "13709  https://t.co/ymsEmlVTTd      4\n",
      "13993  https://t.co/3kFUIOJXEp      4\n",
      "14604  https://t.co/OI39zSAnQ8      4\n",
      "14916  https://t.co/6yVYKIAb2c      4\n",
      "14917  https://t.co/xPcm2Xkj4O      4\n",
      "14951  https://t.co/gu6B4XpqP4      4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Temp\\ipykernel_2060\\2305966469.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  print(words_count_df_copy[words_count_df[\"Word\"].str.startswith('https://t.co')].head(10))\n"
     ]
    }
   ],
   "source": [
    "print(words_count_df_copy[words_count_df[\"Word\"].str.startswith('https://t.co')].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at6lRYZ8A07N"
   },
   "source": [
    "**Ответ:** Так как топ весьма маленький (мало повторений) и по ссылкам не всегда понятна окраска сообщения, в котором была вставлена ссылка, то информация, которую она несёт, в том виде, в котором мы видим сейчас, невелика."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOGdUU1kBU1D"
   },
   "source": [
    "Используем опыт предыдущих экспериментов и напишем собственный токенайзер, улучшив TweetTokenizer. Функция tokenize должна:\n",
    "\n",
    "\n",
    "\n",
    "*   Привести текст в нижний регистр\n",
    "*   Применить TweetTokenizer для  выделения токенов\n",
    "*   Удалить стоп-слова, пунктуацию, токены из одного символа с позицией в таблице Unicode 128 и более,  ссылки на t.co\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ctEsB6xkFrrK",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:07.048482100Z",
     "start_time": "2025-05-12T15:27:06.860765600Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    words_df = pd.DataFrame(words, columns=['Word'])\n",
    "    words_df = words_df[~words_df[\"Word\"].isin(punctuation_list) & ~words_df[\"Word\"].isin(stops)]\n",
    "\n",
    "    words_df = words_df[\n",
    "        (words_df[\"Word\"].str.len() > 1) |\n",
    "        (words_df[\"Word\"].apply(\n",
    "            lambda x: ord(x) if len(x) == 1 else 0\n",
    "        ) <= 127)\n",
    "        ]\n",
    "    \n",
    "    words_df = words_df.loc[~words_df[\"Word\"].str.startswith('https://t.co')]\n",
    "    \n",
    "    return words_df['Word'].array.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwbgtYkJGYym",
    "outputId": "5808765b-3448-45e6-ccc1-7cd65f6371ef",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:07.053482600Z",
     "start_time": "2025-05-12T15:27:06.863682800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['sample', 'text', '@sample_text', '#sampletext']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer('This is sample text!!!! @Sample_text I, \\x92\\x92 https://t.co/sample  #sampletext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wURVABmXHk97"
   },
   "source": [
    "## Задание 3 Векторизация текстов (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H44iXkoHIQfN"
   },
   "source": [
    "Обучите CountVectorizer с использованием custom_tokenizer в качестве токенайзера. Как размер полученного словаря соотносится с размером изначального словаря из начала задания 2?"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = train[\"OriginalTweet\"].array.tolist()\n",
    "# print(corpus[:10])\n",
    "cv = CountVectorizer(tokenizer=custom_tokenizer, lowercase=False) \n",
    "cv.fit(corpus)\n",
    "\n",
    "print(len(cv.vocabulary_))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHn_limQl3BI",
    "outputId": "8e9c1826-319f-4376-f06e-c30c2eb82648",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:31.393883100Z",
     "start_time": "2025-05-12T15:27:06.869874400Z"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45630\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsfmaSGoItUm"
   },
   "source": [
    "**Ответ:** Было ~87K, стало 45.6K, значит мы отсеяли больше мусора, чем могли, значит сделали хорошую работу. Однако в авторские числа я так и не смог попасть. Возможно, из-за обновления списка стоп-слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm6UHNmqKZT0"
   },
   "source": [
    "Посмотрим на какой-нибудь конкретный твитт:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aJVjjfqOJh8m",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:31.400189600Z",
     "start_time": "2025-05-12T15:27:31.393883100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('Products that are shelf-stable &amp; long-lived are in demand as consumers are stockpiling staples in anticipation of state- or self-imposed quarantines. Interest in fresh &amp; artisanal foods is being tested as consumers turn to preserved, shelf-stable products https://t.co/MdDEFzqI39',\n np.int64(0))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 9023\n",
    "train.iloc[ind]['OriginalTweet'], train.loc[ind]['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBMIHBI5KdaS"
   },
   "source": [
    "Автор твитта не доволен ситуацией с едой во Франции и текст имеет резко негативную окраску.\n",
    "\n",
    "Примените обученный CountVectorizer для векторизации данного текста, и попытайтесь определить самый важный токен и самый неважный токен (токен, компонента которого в векторе максимальна/минимальна, без учета 0). Хорошо ли они определились, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7NcAllaEKsJj",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:27:31.461437100Z",
     "start_time": "2025-05-12T15:27:31.400189600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: Shop keepers taking advantage of #Coronavirus boosting prices disproportionately so the most Marginalised suffer who can't afford it #SHAMEONYOU #Wewillremember\n",
      "Слово: #coronavirus, Количество: 1\n",
      "Слово: #shameonyou, Количество: 1\n",
      "Слово: #wewillremember, Количество: 1\n",
      "Слово: advantage, Количество: 1\n",
      "Слово: afford, Количество: 1\n",
      "Слово: boosting, Количество: 1\n",
      "Слово: can't, Количество: 1\n",
      "Слово: disproportionately, Количество: 1\n",
      "Слово: keepers, Количество: 1\n",
      "Слово: marginalised, Количество: 1\n",
      "Слово: prices, Количество: 1\n",
      "Слово: shop, Количество: 1\n",
      "Слово: suffer, Количество: 1\n",
      "Слово: taking, Количество: 1\n"
     ]
    }
   ],
   "source": [
    "X_cv = cv.transform([train.loc[ind]['OriginalTweet']])\n",
    "\n",
    "feature_names = cv.get_feature_names_out()\n",
    "print(\"Текст:\", train.loc[ind]['OriginalTweet'])\n",
    "for i, value in enumerate(X_cv.toarray()[0]):\n",
    "    if value > 0:\n",
    "        print(f\"Слово: {feature_names[i]}, Количество: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpEsl1k_NF4T"
   },
   "source": [
    "**Ответ:** Важность токенов при использовании count vectorizer невозможно понять, т.к. для маленьких текстов (одного из всего корпуса) токен либо встречается, либо нет, они почти всегда будут равнозначны (1), потому что чаще всего будет получаться так, что слово в маленьком предложении встретилось, либо нет, редко будут повторы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4DsEQpLO3J6"
   },
   "source": [
    "Теперь примените TfidfVectorizer и определите самый важный/неважный токены. Хорошо ли определились, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-12T15:28:00.802111900Z",
     "start_time": "2025-05-12T15:27:31.441919700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45630\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = train[\"OriginalTweet\"].array.tolist()\n",
    "# print(corpus[:10])\n",
    "tfidf = TfidfVectorizer(tokenizer=custom_tokenizer) \n",
    "tfidf.fit(corpus)\n",
    "\n",
    "print(len(tfidf.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: Shop keepers taking advantage of #Coronavirus boosting prices disproportionately so the most Marginalised suffer who can't afford it #SHAMEONYOU #Wewillremember\n",
      "Слово: #wewillremember, Коэффициент: 0.37600940431549273\n",
      "Слово: marginalised, Коэффициент: 0.37600940431549273\n",
      "Слово: #shameonyou, Коэффициент: 0.35087104283861403\n",
      "Слово: keepers, Коэффициент: 0.3305754629677927\n",
      "Слово: disproportionately, Коэффициент: 0.3257326813617353\n",
      "Слово: boosting, Коэффициент: 0.29839564736571067\n",
      "Слово: suffer, Коэффициент: 0.27019064399024845\n",
      "Слово: afford, Коэффициент: 0.22843805399348932\n",
      "Слово: advantage, Коэффициент: 0.21241411023288398\n",
      "Слово: can't, Коэффициент: 0.1994785838079851\n",
      "Слово: taking, Коэффициент: 0.18818373193827018\n",
      "Слово: shop, Коэффициент: 0.17165254928767892\n",
      "Слово: prices, Коэффициент: 0.09768255600150214\n",
      "Слово: #coronavirus, Коэффициент: 0.07199655278731801\n"
     ]
    }
   ],
   "source": [
    "X_tv = tfidf.transform([train.loc[ind]['OriginalTweet']])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(\"Текст:\", train.loc[ind]['OriginalTweet'])\n",
    "coefs_sorted = {}\n",
    "for i, value in enumerate(X_tv.toarray()[0]):\n",
    "    if value > 0:\n",
    "        coefs_sorted[feature_names[i]] = value\n",
    "        # print(f\"Слово: {feature_names[i]}, Коэффициент: {value}\")\n",
    "coefs_sorted = dict(sorted(coefs_sorted.items(), key=lambda x: -x[1]))\n",
    "for i in coefs_sorted.keys():\n",
    "    print(f\"Слово: {i}, Коэффициент: {coefs_sorted[i]}\")"
   ],
   "metadata": {
    "id": "uSNzdK3ENGB3",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:28:00.964252300Z",
     "start_time": "2025-05-12T15:28:00.798104800Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYao_UhqQADm"
   },
   "source": [
    "**Ответ:** Теперь каждому токену вполне можно поставить меру важности - число, которое определял новый токенайзер. Он тем больше, чем чаще встречался токен в обучающем тексте и встречался в нашем тестируемом тексте. Сейчас можно сказать, что самые важные токены - #wewillremember, #shameonyou и marginalised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGRJPqfWSesQ"
   },
   "source": [
    "Найдите какой-нибудь положительно окрашенный твитт, где TfidfVectorizer хорошо (полезно для определения окраски) выделяет важный токен, поясните пример.\n",
    "\n",
    "*Подсказка:* явно положительные твитты можно искать при помощи положительных слов (good, great, amazing и т. д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bRbQ2CHiSuJI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "outputId": "c4b34a7d-1076-4e1e-ad5c-9466fd2097c2",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:28:00.969444200Z",
     "start_time": "2025-05-12T15:28:00.913559100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       UserName  ScreenName                       Location     TweetAt  \\\n14334     21202       66154                      Ohio, USA  23-03-2020   \n18296     26062       71014                 Somerville, MA  25-03-2020   \n15010     22046       66998                     Pittsburgh  23-03-2020   \n24347     33577       78529                        Unknown  05-04-2020   \n27273     37248       82200                   Brooklyn, NY  08-04-2020   \n17124     24641       69593                      Miami, FL  25-03-2020   \n11469     17705       62657                Zurich - Dublin  21-03-2020   \n25229     34701       79653                           Bury  06-04-2020   \n24155     33326       78278                   Langtoun, UK  05-04-2020   \n8740      14419       59371                         Nevada  20-03-2020   \n29159     39582       84534                        Alabama  09-04-2020   \n19660     27738       72690                        Unknown  26-03-2020   \n25447     34966       79918                          India  06-04-2020   \n31279     42276       87228         DÃÂ¼sseldorf, Germany  11-04-2020   \n19273     27258       72210  3rd planet in minor solar sys  26-03-2020   \n13355     20006       64958                Los Angeles, CA  22-03-2020   \n30061     40719       85671                        Unknown  10-04-2020   \n\n                                           OriginalTweet  Sentiment  \n14334  @drsmadden A5: I think brands are doing an exc...          1  \n18296  @UMassBoston @UMB_UR_BEST doctoral students wr...          1  \n15010  A massive thank you to @AldiUK ????\\r\\r\\n\\r\\r\\...          1  \n24347  Massive thanks to @waitrose for my delivery of...          1  \n27273  I m currently furloughed from my grocery store...          1  \n17124  So just CNN Or how about the Daily Mail that h...          1  \n11469  we ve put together a list of excellent recipe ...          1  \n25229  Solid Sense range, a name which illustrates @L...          1  \n24155  Local supermarket has a pre-opening hour exclu...          1  \n8740   My betrothed is a type-1 diabetic and manager ...          1  \n29159  This is an excellent idea , this stuff is rare...          1  \n19660  Harris Farm introduces excellent measures to f...          1  \n25447  Our ALL internal #auditor #Training  #course #...          1  \n31279  Pro tip: wonder what to do with all the #toile...          1  \n19273  Talked to the excellent about about transmissi...          1  \n13355  \"This particular edition of Writer's Lounge is...          1  \n30061  The @ftc has compiled an excellent list of res...          1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14334</th>\n      <td>21202</td>\n      <td>66154</td>\n      <td>Ohio, USA</td>\n      <td>23-03-2020</td>\n      <td>@drsmadden A5: I think brands are doing an exc...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18296</th>\n      <td>26062</td>\n      <td>71014</td>\n      <td>Somerville, MA</td>\n      <td>25-03-2020</td>\n      <td>@UMassBoston @UMB_UR_BEST doctoral students wr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15010</th>\n      <td>22046</td>\n      <td>66998</td>\n      <td>Pittsburgh</td>\n      <td>23-03-2020</td>\n      <td>A massive thank you to @AldiUK ????\\r\\r\\n\\r\\r\\...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24347</th>\n      <td>33577</td>\n      <td>78529</td>\n      <td>Unknown</td>\n      <td>05-04-2020</td>\n      <td>Massive thanks to @waitrose for my delivery of...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27273</th>\n      <td>37248</td>\n      <td>82200</td>\n      <td>Brooklyn, NY</td>\n      <td>08-04-2020</td>\n      <td>I m currently furloughed from my grocery store...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17124</th>\n      <td>24641</td>\n      <td>69593</td>\n      <td>Miami, FL</td>\n      <td>25-03-2020</td>\n      <td>So just CNN Or how about the Daily Mail that h...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11469</th>\n      <td>17705</td>\n      <td>62657</td>\n      <td>Zurich - Dublin</td>\n      <td>21-03-2020</td>\n      <td>we ve put together a list of excellent recipe ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25229</th>\n      <td>34701</td>\n      <td>79653</td>\n      <td>Bury</td>\n      <td>06-04-2020</td>\n      <td>Solid Sense range, a name which illustrates @L...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24155</th>\n      <td>33326</td>\n      <td>78278</td>\n      <td>Langtoun, UK</td>\n      <td>05-04-2020</td>\n      <td>Local supermarket has a pre-opening hour exclu...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8740</th>\n      <td>14419</td>\n      <td>59371</td>\n      <td>Nevada</td>\n      <td>20-03-2020</td>\n      <td>My betrothed is a type-1 diabetic and manager ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29159</th>\n      <td>39582</td>\n      <td>84534</td>\n      <td>Alabama</td>\n      <td>09-04-2020</td>\n      <td>This is an excellent idea , this stuff is rare...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19660</th>\n      <td>27738</td>\n      <td>72690</td>\n      <td>Unknown</td>\n      <td>26-03-2020</td>\n      <td>Harris Farm introduces excellent measures to f...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25447</th>\n      <td>34966</td>\n      <td>79918</td>\n      <td>India</td>\n      <td>06-04-2020</td>\n      <td>Our ALL internal #auditor #Training  #course #...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>31279</th>\n      <td>42276</td>\n      <td>87228</td>\n      <td>DÃÂ¼sseldorf, Germany</td>\n      <td>11-04-2020</td>\n      <td>Pro tip: wonder what to do with all the #toile...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19273</th>\n      <td>27258</td>\n      <td>72210</td>\n      <td>3rd planet in minor solar sys</td>\n      <td>26-03-2020</td>\n      <td>Talked to the excellent about about transmissi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13355</th>\n      <td>20006</td>\n      <td>64958</td>\n      <td>Los Angeles, CA</td>\n      <td>22-03-2020</td>\n      <td>\"This particular edition of Writer's Lounge is...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30061</th>\n      <td>40719</td>\n      <td>85671</td>\n      <td>Unknown</td>\n      <td>10-04-2020</td>\n      <td>The @ftc has compiled an excellent list of res...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['OriginalTweet'].apply(lambda x: 'excellent' in x) & (train['Sentiment'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ind = 15010\n",
    "\n",
    "X_tv = tfidf.transform([train.loc[ind]['OriginalTweet']])\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(\"Текст:\", train.loc[ind]['OriginalTweet'])\n",
    "coefs_sorted = {}\n",
    "for i, value in enumerate(X_tv.toarray()[0]):\n",
    "    if value > 0:\n",
    "        coefs_sorted[feature_names[i]] = value\n",
    "        # print(f\"Слово: {feature_names[i]}, Коэффициент: {value}\")\n",
    "coefs_sorted = dict(sorted(coefs_sorted.items(), key=lambda x: -x[1]))\n",
    "for i in coefs_sorted.keys():\n",
    "    print(f\"Слово: {i}, Коэффициент: {coefs_sorted[i]}\")"
   ],
   "metadata": {
    "id": "jSjbKPCWk87K",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:28:01.075055700Z",
     "start_time": "2025-05-12T15:28:00.930853800Z"
    }
   },
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: A massive thank you to @AldiUK ????\r\n",
      "\r\n",
      "Thanks for sticking to the 4 items limit. And your excellent staff.\r\n",
      "\r\n",
      "In your own way. You are saving lives too.\r\n",
      "\r\n",
      "This is why I love #Aldi\r\n",
      "On both sides of the Atlantic. Not just for the prices.\r\n",
      "#StayAtHomeSaveLives #COVID?19 #coronavirus\n",
      "Слово: atlantic, Коэффициент: 0.3068388861914154\n",
      "Слово: sides, Коэффициент: 0.3068388861914154\n",
      "Слово: sticking, Коэффициент: 0.29837891836670993\n",
      "Слово: #aldi, Коэффициент: 0.28869471084494247\n",
      "Слово: #stayathomesavelives, Коэффициент: 0.2698564308762369\n",
      "Слово: excellent, Коэффициент: 0.25899094841311204\n",
      "Слово: @aldiuk, Коэффициент: 0.252660568423545\n",
      "Слово: saving, Коэффициент: 0.24346072726591966\n",
      "Слово: massive, Коэффициент: 0.21834587531727861\n",
      "Слово: limit, Коэффициент: 0.2069697096060605\n",
      "Слово: love, Коэффициент: 0.20087351947012985\n",
      "Слово: lives, Коэффициент: 0.1913964687961125\n",
      "Слово: thanks, Коэффициент: 0.17978165137592952\n",
      "Слово: 4, Коэффициент: 0.1775401880516569\n",
      "Слово: items, Коэффициент: 0.16451639882165303\n",
      "Слово: thank, Коэффициент: 0.15962915402471817\n",
      "Слово: way, Коэффициент: 0.15902801516586565\n",
      "Слово: staff, Коэффициент: 0.15745660143387508\n",
      "Слово: #covid, Коэффициент: 0.15536232637471506\n",
      "Слово: prices, Коэффициент: 0.09066857659290842\n",
      "Слово: 19, Коэффициент: 0.07433761835658013\n",
      "Слово: #coronavirus, Коэффициент: 0.0668269261987978\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTv9ST2_U6NA"
   },
   "source": [
    "**Ответ:** Видим в топе среди обычных нейтральных слов или тегов еще положительно окрашенные слова - love, thanks, excellent. Так как они в вершине топа, то они прямо намекают на нужный стиль твита."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVEuZm8BHms6"
   },
   "source": [
    "## Задание 4 Обучение первых моделей (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JADkO3sfXdOG"
   },
   "source": [
    "Примените оба векторайзера для получения матриц с признаками текстов.  Выделите целевую переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-12T15:28:01.165008400Z",
     "start_time": "2025-05-12T15:28:00.966851400Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train[\"Sentiment\"], test[\"Sentiment\"]\n",
    "\n",
    "train.drop([\"Sentiment\"], axis=1, inplace=True)\n",
    "test.drop([\"Sentiment\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23410, 45630) (23410, 45630)\n"
     ]
    }
   ],
   "source": [
    "corpus = train[\"OriginalTweet\"].array.tolist()\n",
    "\n",
    "X_cv = cv.transform(corpus)\n",
    "X_tv = tfidf.transform(corpus)\n",
    "\n",
    "print(X_cv.shape, X_tv.shape)"
   ],
   "metadata": {
    "id": "DguoiXhCX2oN",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:29:41.960569Z",
     "start_time": "2025-05-12T15:28:00.975154700Z"
    }
   },
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FX1KSOfYSx4"
   },
   "source": [
    "Обучите логистическую регрессию на векторах из обоих векторайзеров. Посчитайте долю правильных ответов на обучающих и тестовых данных. Какой векторайзер показал лучший результат? Что можно сказать о моделях?\n",
    "\n",
    "Используйте `sparse` матрицы (после векторизации), не превращайте их в `numpy.ndarray` или `pd.DataFrame` - может не хватить памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-Tb3eh8UXJ6v",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:29:42.756911Z",
     "start_time": "2025-05-12T15:29:41.960569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()",
      "text/html": "<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg_cv = LogisticRegression()\n",
    "logreg_cv.fit(X_cv, y_train)\n",
    "\n",
    "logreg_tv = LogisticRegression()\n",
    "logreg_tv.fit(X_tv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      train test\n",
      "cv    0.984 0.872\n",
      "tfidf 0.926 0.852\n"
     ]
    }
   ],
   "source": [
    "test_corpus = test[\"OriginalTweet\"].array.tolist()\n",
    "\n",
    "y_test_pred_cv = logreg_cv.predict(cv.transform(test_corpus))\n",
    "y_test_pred_tv = logreg_tv.predict(tfidf.transform(test_corpus))\n",
    "\n",
    "acc_test_cv = accuracy_score(y_test, y_test_pred_cv)\n",
    "acc_test_tv = accuracy_score(y_test, y_test_pred_tv)\n",
    "\n",
    "y_train_pred_cv = logreg_cv.predict(cv.transform(corpus))\n",
    "y_train_pred_tv = logreg_tv.predict(tfidf.transform(corpus))\n",
    "\n",
    "acc_train_cv = accuracy_score(y_train, y_train_pred_cv)\n",
    "acc_train_tv = accuracy_score(y_train, y_train_pred_tv)\n",
    "\n",
    "print(f\"      train test\")\n",
    "print(f\"cv    {round(acc_train_cv, 3)} {round(acc_test_cv, 3)}\")\n",
    "print(f\"tfidf {round(acc_train_tv, 3)} {round(acc_test_tv, 3)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-12T15:31:03.541200300Z",
     "start_time": "2025-05-12T15:29:42.756911Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8y_wO7rCmv7K"
   },
   "source": [
    "**Ответ:** Как мы видим, ошибка на обучающей выборке близка к нулю (на cv меньше, чем на tfidf), на тестах accuracy +- около 0.86 с таким же небольшим преимуществом у cv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSOR1i3mjrys"
   },
   "source": [
    "## Задание 5 Стемминг (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6ONBWNPjuq-"
   },
   "source": [
    "Для уменьшения словаря можно использовать стемминг.\n",
    "\n",
    "Модифицируйте написанный токенайзер, добавив в него стемминг с использованием SnowballStemmer. Обучите Count- и Tfidf- векторайзеры. Как изменился размер словаря?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "oVfA2-iMkQBb",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:31:03.546920700Z",
     "start_time": "2025-05-12T15:31:03.542204900Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def custom_stem_tokenizer(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    snow_stemmer = SnowballStemmer(language='english')\n",
    "    set_of_trash = set(punctuation_list).union(stops)\n",
    "\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    words_df = pd.DataFrame(words, columns=['Word'])\n",
    "    \n",
    "    # punctuation and stops\n",
    "    words_df = words_df[~words_df[\"Word\"].isin(set_of_trash)]\n",
    "    \n",
    "    # chars > 127\n",
    "    words_df = words_df[\n",
    "        (words_df[\"Word\"].str.len() > 1) |\n",
    "        (words_df[\"Word\"].apply(\n",
    "            lambda x: ord(x) if len(x) == 1 else 0\n",
    "        ) <= 127)\n",
    "        ]\n",
    "    \n",
    "    # links\n",
    "    words_df = words_df.loc[~words_df[\"Word\"].str.startswith('https://t.co')]\n",
    "    \n",
    "    # stemming\n",
    "    words_df[\"Word\"] = words_df[\"Word\"].apply(lambda x: snow_stemmer.stem(x))\n",
    "    return words_df['Word'].array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QmrjYtqnlPd",
    "outputId": "cd91291d-9676-4611-9fc4-28afaed58963",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:31:03.577146200Z",
     "start_time": "2025-05-12T15:31:03.545845400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['sampl', 'text', '@sample_text', '#sampletext', 'ad', 'word', 'check', 'stem']"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stem_tokenizer(\n",
    "    'This is sample text!!!! @Sample_text I, \\x92\\x92 https://t.co/sample  #sampletext adding more words to check stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAvUTmaplzOS",
    "outputId": "566207fe-183b-4ed6-d333-f86f0cc9ae38",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:31:35.521781700Z",
     "start_time": "2025-05-12T15:31:03.554786300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36916\n"
     ]
    }
   ],
   "source": [
    "corpus = train[\"OriginalTweet\"].array.tolist()\n",
    "cv = CountVectorizer(tokenizer=custom_stem_tokenizer, lowercase=False) \n",
    "cv.fit(corpus)\n",
    "\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oyzs5TaAoHP6"
   },
   "source": [
    "**Ответ** Словарь уменьшился, видимо, потому что после стемминга слова с одинаковым корнем стали учитываться как одни и те же, т.е. мы объединили слова с одним корнем, но разными суффиксами/окончаниями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OkncHI8oRmd"
   },
   "source": [
    "Обучите логистическую регрессию с использованием обоих векторайзеров. Изменилось ли качество? Есть ли смысл применять стемминг?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-12T15:54:01.244545Z",
     "start_time": "2025-05-12T15:51:00.054769300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dmitriy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "LogisticRegression()",
      "text/html": "<style>#sk-container-id-4 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-4 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-4 pre {\n  padding: 0;\n}\n\n#sk-container-id-4 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-4 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-4 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-4 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-4 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-4 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-4 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-4 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-4 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-4 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-4 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-4 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-4 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-4 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-4 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n#sk-container-id-4 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-4 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-4 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-4 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-4 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-4 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-4 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-4 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus = train[\"OriginalTweet\"].array.tolist()\n",
    "test_corpus = test[\"OriginalTweet\"].array.tolist()\n",
    "\n",
    "logreg_cv = LogisticRegression()\n",
    "logreg_tv = LogisticRegression()\n",
    "tfidf = TfidfVectorizer(tokenizer=custom_stem_tokenizer)\n",
    "cv = CountVectorizer(tokenizer=custom_stem_tokenizer)\n",
    "\n",
    "cv.fit(train_corpus)\n",
    "tfidf.fit(train_corpus)\n",
    "\n",
    "X_cv = cv.transform(train_corpus)\n",
    "X_tv = tfidf.transform(train_corpus)\n",
    "\n",
    "logreg_cv.fit(X_cv, y_train)\n",
    "logreg_tv.fit(X_tv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36916\n",
      "36916\n"
     ]
    },
    {
     "data": {
      "text/plain": "(None, None)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(cv.vocabulary_)), print(len(tfidf.vocabulary_))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-12T15:54:16.852784100Z",
     "start_time": "2025-05-12T15:54:16.833780Z"
    }
   },
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      train test\n",
      "cv    0.971 0.871\n",
      "tfidf 0.917 0.859\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_cv = logreg_cv.predict(cv.transform(test_corpus))\n",
    "y_test_pred_tv = logreg_tv.predict(tfidf.transform(test_corpus))\n",
    "\n",
    "acc_test_cv = accuracy_score(y_test, y_test_pred_cv)\n",
    "acc_test_tv = accuracy_score(y_test, y_test_pred_tv)\n",
    "\n",
    "y_train_pred_cv = logreg_cv.predict(cv.transform(corpus))\n",
    "y_train_pred_tv = logreg_tv.predict(tfidf.transform(corpus))\n",
    "\n",
    "acc_train_cv = accuracy_score(y_train, y_train_pred_cv)\n",
    "acc_train_tv = accuracy_score(y_train, y_train_pred_tv)\n",
    "\n",
    "print(f\"      train test\")\n",
    "print(f\"cv    {round(acc_train_cv, 3)} {round(acc_test_cv, 3)}\")\n",
    "print(f\"tfidf {round(acc_train_tv, 3)} {round(acc_test_tv, 3)}\")"
   ],
   "metadata": {
    "id": "ykZJPphEoZ5W",
    "ExecuteTime": {
     "end_time": "2025-05-12T15:57:13.225651400Z",
     "start_time": "2025-05-12T15:54:21.775516800Z"
    }
   },
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCRlrODro0h8"
   },
   "source": [
    "**Ответ:** На удивление, стемминг показал чуть более худшие результаты чем без него, улучшился совсем немного только test tfidf, все остальные показания практически не изменились, либо упали "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYWGQNEDqLC-"
   },
   "source": [
    "## Задание 6 Работа с частотами (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hq-tl5mqUSn"
   },
   "source": [
    "Еще один способ уменьшить количество признаков - это использовать параметры min_df и max_df при построении векторайзера  эти параметры помогают ограничить требуемую частоту встречаемости токена в документах.\n",
    "\n",
    "По умолчанию берутся все токены, которые встретились хотя бы один раз.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1SiD4DE3WZ2"
   },
   "source": [
    "Подберите max_df такой, что размер словаря будет 36651 (на 1 меньше, чем было). Почему параметр получился такой большой/маленький?"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cv_df = CountVectorizer(tokenizer=custom_stem_tokenizer,\n",
    "                        max_df=  # -- YOUR CODE HERE --\n",
    "                        ).fit(\n",
    "    # -- YOUR CODE HERE --\n",
    ")\n",
    "print(len(cv_df.vocabulary_))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3YLb8PViExb",
    "outputId": "b6d67654-d232-4e11-a5ca-6f2145053e98"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "36651\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "tyEpkJUkjnuK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdZYoGZR4UsA"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gRIUaB1u32f"
   },
   "source": [
    "Подберите min_df (используйте дефолтное значение max_df) в CountVectorizer таким образом, чтобы размер словаря был 3700 токенов (при использовании токенайзера со стеммингом), а качество осталось таким же, как и было. Что можно сказать о результатах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSnMJkn9XmsT",
    "outputId": "e0d8eb21-e5d7-46b4-e1d1-4b1ae220e9a0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3700\n"
     ]
    }
   ],
   "source": [
    "cv_df = CountVectorizer(tokenizer=custom_stem_tokenizer,\n",
    "                        min_df=  # -- YOUR CODE HERE --\n",
    "                        ).fit(\n",
    "    # -- YOUR CODE HERE --\n",
    ")\n",
    "print(len(cv_df.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "mvMDwpdfjm8Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fGYpUIZx0fk"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "В предыдущих заданиях признаки не скалировались. Отскалируйте данные (при словаре размера 3.7 тысяч, векторизованные CountVectorizer), обучите логистическую регрессию, посмотрите качество и выведите `barplot`, содержащий по 10 токенов, с наибольшим по модулю положительными/отрицательными весами. Что можно сказать об этих токенах?"
   ],
   "metadata": {
    "id": "Gx_h_-inKbBl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "KBATXJX6LG9q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ],
   "metadata": {
    "id": "ThcEfzY1LHET"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktJVOdrIHq7B"
   },
   "source": [
    "## Задание 7 Другие признаки (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt3jRCZ2H0Og"
   },
   "source": [
    "Мы были сконцентрированы на работе с текстами твиттов и не использовали другие признаки - имена пользователя, дату и местоположение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52wjewCCo_di"
   },
   "source": [
    "Изучите признаки UserName и ScreenName. полезны ли они? Если полезны, то закодируйте их, добавьте к матрице с отскалированными признаками, обучите логистическую регрессию, замерьте качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63thouYZptj6"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8_qR-gnpT3a"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ythEcFSkt7y3"
   },
   "source": [
    "Изучите признак TweetAt в обучающей выборке: преобразуйте его к типу datetime и нарисуйте его гистограмму с разделением по цвету на основе целевой переменной. Полезен ли он? Если полезен, то закодируйте его, добавьте к матрице с отскалированными признаками, обучите логистическую регрессию, замерьте качество."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "Lxb_k0JLirNv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IdLBdpQxM-G"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поработайте с признаком Location в обучающей выборке. Сколько уникальных значений?"
   ],
   "metadata": {
    "id": "r2JtRPhNP6qx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "xYQZQ1FRNpoe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Постройте гистограмму топ-10 по популярности местоположений (исключая Unknown)"
   ],
   "metadata": {
    "id": "6k4JwpRTQISa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "J91YkhegJ0mz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видно, что многие местоположения включают в себя более точное название места, чем другие (Например, у некоторых стоит London, UK; а у некоторых просто UK или United Kingdom).\n",
    "\n",
    "Создайте новый признак WiderLocation, который содержит самое широкое местоположение (например, из London, UK должно получиться UK). Сколько уникальных категорий теперь? Постройте аналогичную гистограмму."
   ],
   "metadata": {
    "id": "ZOsv3lODTfYB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "mSkow6acOMyD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Закодируйте признак WiderLocation с помощью OHE таким образом, чтобы создались только столбцы для местоположений, которые встречаются более одного раза. Сколько таких значений?\n"
   ],
   "metadata": {
    "id": "cgyWrD2eVfff"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "SeJBfBWgPvg_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Добавьте этот признак к матрице отскалированных текстовых признаков, обучите логистическую регрессию, замерьте качество. Как оно изменилось? Оказался ли признак полезным?\n",
    "\n",
    "\n",
    "*Подсказка:* используйте параметр `categories` в энкодере."
   ],
   "metadata": {
    "id": "ZyMX5kZuimPK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# -- YOUR CODE HERE --"
   ],
   "metadata": {
    "id": "EO1jNPeeim7A"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ],
   "metadata": {
    "id": "7dHsGlDRYUQt"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
